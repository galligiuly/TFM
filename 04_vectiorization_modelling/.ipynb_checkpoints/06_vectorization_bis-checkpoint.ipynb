{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/78/7cf94b3d0961b1a3036ba351c7fdc04170baa73d20fcb41240da214c83fd/google_cloud_storage-1.23.0-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 5.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.0.3 (from google-cloud-storage)\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/f0/084f598629db8e6ec3627688723875cdb03637acb6d86999bb105a71df64/google_cloud_core-1.0.3-py2.py3-none-any.whl\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-storage)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/9e/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098/google_resumable_media-0.5.0-py2.py3-none-any.whl\n",
      "Collecting google-auth>=1.2.0 (from google-cloud-storage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 5.7MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting google-api-core<2.0.0dev,>=1.14.0 (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/3a/c528ef37f48d6ffba16f0f3c0426456ba21e0dd32be9c61a2ade93e07faa/google_api_core-1.14.3-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 4.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /Applications/anaconda3/lib/python3.6/site-packages (from google-resumable-media<0.6dev,>=0.5.0->google-cloud-storage) (1.12.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.2.0->google-cloud-storage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 4.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<3.2,>=2.0.0 (from google-auth>=1.2.0->google-cloud-storage)\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth>=1.2.0->google-cloud-storage)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /Applications/anaconda3/lib/python3.6/site-packages (from google-auth>=1.2.0->google-cloud-storage) (40.6.3)\n",
      "Requirement already satisfied: pytz in /Applications/anaconda3/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2018.7)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Applications/anaconda3/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2.21.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /Applications/anaconda3/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (3.6.1)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/ee/e59e74ecac678a14d6abefb9054f0bbcb318a6452a30df3776f133886d7d/googleapis-common-protos-1.6.0.tar.gz\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 6.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Applications/anaconda3/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Applications/anaconda3/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2019.6.16)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Applications/anaconda3/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (1.24.1)\n",
      "Building wheels for collected packages: googleapis-common-protos\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/giuliagalli/Library/Caches/pip/wheels/9e/3d/a2/1bec8bb7db80ab3216dbc33092bb7ccd0debfb8ba42b5668d5\n",
      "Successfully built googleapis-common-protos\n",
      "Installing collected packages: pyasn1, pyasn1-modules, cachetools, rsa, google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-storage\n",
      "Successfully installed cachetools-3.1.1 google-api-core-1.14.3 google-auth-1.7.1 google-cloud-core-1.0.3 google-cloud-storage-1.23.0 google-resumable-media-0.5.0 googleapis-common-protos-1.6.0 pyasn1-0.4.8 pyasn1-modules-0.2.7 rsa-4.0\n"
     ]
    }
   ],
   "source": [
    "# !curl https://sdk.cloud.google.com | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?code_challenge=6akuZxzqFXVpl4K92YfoRirEV6hyYXrIOwsg7JJhBMU&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
      "\n",
      "\n",
      "\u001b[1;33mWARNING:\u001b[0m `gcloud auth login` no longer writes application default credentials.\n",
      "If you need to use ADC, see:\n",
      "  gcloud auth application-default --help\n",
      "\n",
      "You are now logged in as [galli.giuly@gmail.com].\n",
      "Your current project is [None].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "# connecting to google cloud\n",
    "\n",
    "!/Users/giuliagalli/google-cloud-sdk/bin/gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_tokenized_df.pkl...\n",
      "- [1 files][  1.6 GiB/  1.6 GiB]    7.7 MiB/s                                   \n",
      "Operation completed over 1 objects/1.6 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!/Users/giuliagalli/google-cloud-sdk/bin/gsutil cp gs://reddit_final_results/comments_posts_tokenized_df.pkl .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/27/6fdcddfbce1963989eb527f0ba4749829509c0172c275806cffd5a7e1776/gensim-3.8.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.7MB 541kB/s eta 0:00:01    89% |████████████████████████████▋   | 22.1MB 23.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /Applications/anaconda3/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /Applications/anaconda3/lib/python3.6/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /Applications/anaconda3/lib/python3.6/site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /Applications/anaconda3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Applications/anaconda3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/9b/11/7e6470f5d7d7d23fc5eaae64f8a3f4b844ca08234cc3207df027267b65c4/boto3-1.10.26-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Applications/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /Applications/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Applications/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Collecting botocore<1.14.0,>=1.13.26 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/93/ea2ec042794dfda186348df02c6057223a8bbc21c055124fbe3e16925441/botocore-1.13.26-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /Applications/anaconda3/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.26->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /Applications/anaconda3/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.26->boto3->smart-open>=1.8.1->gensim) (2.7.5)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/giuliagalli/Library/Caches/pip/wheels/ab/10/93/5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.10.26 botocore-1.13.26 gensim-3.8.1 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giuliagalli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk.data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_posts_tokenized_df = pd.read_pickle(\"/Users/giuliagalli/Documents/GitHub/TFM/04_vectiorization/comments_posts_tokenized_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'subreddit', 'body', 'subreddit_id'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_posts_tokenized_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aww</td>\n",
       "      <td>[doubl, multipl, sub, approach, good, ya, mate]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aww</td>\n",
       "      <td>[2nd, meatbal, serious, chunki, english, bulld...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>aww</td>\n",
       "      <td>[thought]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1 subreddit                                               body  \\\n",
       "0             0       aww    [doubl, multipl, sub, approach, good, ya, mate]   \n",
       "1             1       aww  [2nd, meatbal, serious, chunki, english, bulld...   \n",
       "2             2       aww                                          [thought]   \n",
       "\n",
       "   subreddit_id  \n",
       "0             3  \n",
       "1             3  \n",
       "2             3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_posts_tokenized_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['body', 'subreddit_id']\n"
     ]
    }
   ],
   "source": [
    "categories = list(comments_posts_tokenized_df.columns.values)[2:]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments_posts_tokenized_df['body']\n",
    "y = comments_posts_tokenized_df['subreddit_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definint a fucntion that slit the dataset into three subsets: train, val and test\n",
    "def train_dev_test_split(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42, stratify=y_val)\n",
    "    return dict(X_train=X_train, \n",
    "                X_val=X_val, \n",
    "                X_test=X_test, \n",
    "                y_train=y_train, \n",
    "                y_val=y_val, \n",
    "                y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = train_dev_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_train', 'X_val', 'X_test', 'y_train', 'y_val', 'y_test'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = nltk.word_tokenize(comments_posts_tokenized_df['body'].to_string())\n",
    "bag_of_words = list(dict.fromkeys(bag_of_words))\n",
    "print(bag_of_words[:10])\n",
    "print(len(bag_of_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectors = CountVectorizer(ngram_range=(1,2), strip_accents=\"unicode\", encoding='utf-8')\n",
    "\n",
    "train_term_count = count_vectors.fit_transform(df_split['X_train'].astype(str))\n",
    "test_term_count = count_vectors.transform(df_split['X_val'].astype(str))\n",
    "test_term_count = count_vectors.transform(df_split['X_val'].astype(str))\n",
    "\n",
    "train_term_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectors.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = train_term_count[0, :].toarray()\n",
    "\n",
    "# compute the most common value\n",
    "c = Counter(v[0])\n",
    "print(c.most_common(1))\n",
    "\n",
    "print(f\"min: {v.min()}\")\n",
    "print(f\"avg: {v.mean()}\")\n",
    "print(f\"max: {v.max()}\")\n",
    "print(f\"std: {v.std()}\")\n",
    "print(f\"mode: {c.most_common(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(count_vectors.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors = TfidfVectorizer(ngram_range=(1,2),analyzer='word',strip_accents=\"unicode\")\n",
    "\n",
    "train_term_tfidf = tfidf_vectors.fit_transform(df_split['X_train'].astype(str))\n",
    "val_term_tfidf = tfidf_vectors.transform(df_split['X_val'].astype(str))\n",
    "test_term_tfidf = tfidf_vectors.transform(df_split['X_test'].astype(str))\n",
    "\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 00',\n",
       " '00 000',\n",
       " '00 0030',\n",
       " '00 01',\n",
       " '00 01100',\n",
       " '00 02',\n",
       " '00 04',\n",
       " '00 06',\n",
       " '00 10',\n",
       " '00 100',\n",
       " '00 11',\n",
       " '00 12',\n",
       " '00 1214',\n",
       " '00 16',\n",
       " '00 18',\n",
       " '00 190',\n",
       " '00 1900',\n",
       " '00 1i',\n",
       " '00 1just',\n",
       " '00 20',\n",
       " '00 207',\n",
       " '00 21',\n",
       " '00 22',\n",
       " '00 24',\n",
       " '00 2d',\n",
       " '00 310',\n",
       " '00 350',\n",
       " '00 3ellington',\n",
       " '00 40',\n",
       " '00 411',\n",
       " '00 610',\n",
       " '00 80',\n",
       " '00 90s',\n",
       " '00 91',\n",
       " '00 99',\n",
       " '00 absolut',\n",
       " '00 actual',\n",
       " '00 add',\n",
       " '00 agent',\n",
       " '00 aim',\n",
       " '00 alex',\n",
       " '00 alot',\n",
       " '00 american',\n",
       " '00 amv',\n",
       " '00 annoy',\n",
       " '00 anymor',\n",
       " '00 apr',\n",
       " '00 astonish',\n",
       " '00 aw',\n",
       " '00 axe',\n",
       " '00 ayrshir',\n",
       " '00 bad',\n",
       " '00 basic',\n",
       " '00 battl',\n",
       " '00 bbc',\n",
       " '00 believ',\n",
       " '00 benzodiazepin',\n",
       " '00 best',\n",
       " '00 better',\n",
       " '00 birth',\n",
       " '00 blkpg',\n",
       " '00 bob',\n",
       " '00 boost',\n",
       " '00 bpmprigioni',\n",
       " '00 breakthrough',\n",
       " '00 buck',\n",
       " '00 buckshot',\n",
       " '00 byte',\n",
       " '00 calori',\n",
       " '00 came',\n",
       " '00 camri',\n",
       " '00 cap',\n",
       " '00 caputo',\n",
       " '00 carb',\n",
       " '00 carreer',\n",
       " '00 case',\n",
       " '00 casualti',\n",
       " '00 caus',\n",
       " '00 center',\n",
       " '00 chanc',\n",
       " '00 charg',\n",
       " '00 clear',\n",
       " '00 collect',\n",
       " '00 comedi',\n",
       " '00 conclud',\n",
       " '00 congress',\n",
       " '00 consist',\n",
       " '00 coordin',\n",
       " '00 corolla',\n",
       " '00 cp3',\n",
       " '00 crazi',\n",
       " '00 crskyfal',\n",
       " '00 crucial',\n",
       " '00 cultur',\n",
       " '00 cunt',\n",
       " '00 current',\n",
       " '00 cuzz',\n",
       " '00 dad',\n",
       " '00 day',\n",
       " '00 death',\n",
       " '00 depend',\n",
       " '00 depp',\n",
       " '00 despit',\n",
       " '00 dice',\n",
       " '00 die',\n",
       " '00 draw',\n",
       " '00 drove',\n",
       " '00 dude',\n",
       " '00 dumb',\n",
       " '00 dunno',\n",
       " '00 earli',\n",
       " '00 edit',\n",
       " '00 effici',\n",
       " '00 elect',\n",
       " '00 eminem',\n",
       " '00 end',\n",
       " '00 enforc',\n",
       " '00 enorm',\n",
       " '00 ensur',\n",
       " '00 equal',\n",
       " '00 eu',\n",
       " '00 eventu',\n",
       " '00 exact',\n",
       " '00 explain',\n",
       " '00 explan',\n",
       " '00 eye',\n",
       " '00 face',\n",
       " '00 fact',\n",
       " '00 fail',\n",
       " '00 fan',\n",
       " '00 felix',\n",
       " '00 figur',\n",
       " '00 final',\n",
       " '00 find',\n",
       " '00 fine',\n",
       " '00 fli',\n",
       " '00 flip',\n",
       " '00 flour',\n",
       " '00 ft',\n",
       " '00 fusion',\n",
       " '00 gabriel',\n",
       " '00 game',\n",
       " '00 gaug',\n",
       " '00 gel',\n",
       " '00 generat',\n",
       " '00 german',\n",
       " '00 go',\n",
       " '00 goal',\n",
       " '00 good',\n",
       " '00 got',\n",
       " '00 great',\n",
       " '00 gt',\n",
       " '00 guess',\n",
       " '00 guy',\n",
       " '00 haha',\n",
       " '00 harden',\n",
       " '00 hate',\n",
       " '00 hear',\n",
       " '00 heard',\n",
       " '00 hell',\n",
       " '00 help',\n",
       " '00 high',\n",
       " '00 hope',\n",
       " '00 hour',\n",
       " '00 ice',\n",
       " '00 idea',\n",
       " '00 imo',\n",
       " '00 import',\n",
       " '00 inflat',\n",
       " '00 instead',\n",
       " '00 internet',\n",
       " '00 iron',\n",
       " '00 jordan',\n",
       " '00 kept',\n",
       " '00 kid',\n",
       " '00 kind',\n",
       " '00 kinda',\n",
       " '00 know',\n",
       " '00 knowledg',\n",
       " '00 laker',\n",
       " '00 lead',\n",
       " '00 lebron',\n",
       " '00 like',\n",
       " '00 limit',\n",
       " '00 list',\n",
       " '00 littl',\n",
       " '00 london',\n",
       " '00 longer',\n",
       " '00 look',\n",
       " '00 make',\n",
       " '00 malm',\n",
       " '00 massiv',\n",
       " '00 match',\n",
       " '00 math',\n",
       " '00 matrix',\n",
       " '00 mean',\n",
       " '00 memphi',\n",
       " '00 merced',\n",
       " '00 minor',\n",
       " '00 minut',\n",
       " '00 misogynist',\n",
       " '00 miss',\n",
       " '00 mom',\n",
       " '00 morph',\n",
       " '00 ms',\n",
       " '00 multiplay',\n",
       " '00 mvp',\n",
       " '00 nba',\n",
       " '00 negat',\n",
       " '00 nevermind',\n",
       " '00 new',\n",
       " '00 nintendo',\n",
       " '00 nippl',\n",
       " '00 nowaday',\n",
       " '00 nt',\n",
       " '00 number',\n",
       " '00 opiat',\n",
       " '00 opinion',\n",
       " '00 pant',\n",
       " '00 part',\n",
       " '00 past',\n",
       " '00 peopl',\n",
       " '00 percent',\n",
       " '00 person',\n",
       " '00 philosophi',\n",
       " '00 photo',\n",
       " '00 pivot',\n",
       " '00 place',\n",
       " '00 playoff',\n",
       " '00 pm',\n",
       " '00 point',\n",
       " '00 pop',\n",
       " '00 portland',\n",
       " '00 pretti',\n",
       " '00 prime',\n",
       " '00 probabl',\n",
       " '00 program',\n",
       " '00 progress',\n",
       " '00 promot',\n",
       " '00 protest',\n",
       " '00 proxi',\n",
       " '00 qualifi',\n",
       " '00 quick',\n",
       " '00 rabsoluteunit',\n",
       " '00 radio',\n",
       " '00 raiser',\n",
       " '00 raptor',\n",
       " '00 rare',\n",
       " '00 read',\n",
       " '00 reason',\n",
       " '00 recess',\n",
       " '00 redid',\n",
       " '00 ref',\n",
       " '00 relat',\n",
       " '00 relev',\n",
       " '00 remark',\n",
       " '00 rememb',\n",
       " '00 remov',\n",
       " '00 report',\n",
       " '00 result',\n",
       " '00 reward',\n",
       " '00 right',\n",
       " '00 road',\n",
       " '00 roma',\n",
       " '00 round',\n",
       " '00 ruin',\n",
       " '00 run',\n",
       " '00 rush',\n",
       " '00 s10',\n",
       " '00 said',\n",
       " '00 say',\n",
       " '00 season',\n",
       " '00 second',\n",
       " '00 sedit',\n",
       " '00 seen',\n",
       " '00 segway',\n",
       " '00 seri',\n",
       " '00 server',\n",
       " '00 shaq',\n",
       " '00 shaqstamina',\n",
       " '00 shitti',\n",
       " '00 shrug',\n",
       " '00 si',\n",
       " '00 sif',\n",
       " '00 skareem',\n",
       " '00 slot',\n",
       " '00 spent',\n",
       " '00 spiderman',\n",
       " '00 spur',\n",
       " '00 spurssun',\n",
       " '00 start',\n",
       " '00 stat',\n",
       " '00 status',\n",
       " '00 sthis',\n",
       " '00 straightedg',\n",
       " '00 stress',\n",
       " '00 student',\n",
       " '00 superson',\n",
       " '00 superstar',\n",
       " '00 sure',\n",
       " '00 surpris',\n",
       " '00 swear',\n",
       " '00 taken',\n",
       " '00 talent',\n",
       " '00 tatum',\n",
       " '00 team',\n",
       " '00 teemo',\n",
       " '00 teen',\n",
       " '00 ten',\n",
       " '00 thank',\n",
       " '00 thing',\n",
       " '00 think',\n",
       " '00 thought',\n",
       " '00 time',\n",
       " '00 timelin',\n",
       " '00 token',\n",
       " '00 toler',\n",
       " '00 tone',\n",
       " '00 track',\n",
       " '00 trajectori',\n",
       " '00 trash',\n",
       " '00 tri',\n",
       " '00 trl',\n",
       " '00 true',\n",
       " '00 tt',\n",
       " '00 turkey',\n",
       " '00 unbreak',\n",
       " '00 undefin',\n",
       " '00 underfund',\n",
       " '00 usdt',\n",
       " '00 vault',\n",
       " '00 video',\n",
       " '00 vorpbarn',\n",
       " '00 vs',\n",
       " '00 want',\n",
       " '00 wast',\n",
       " '00 watch',\n",
       " '00 way',\n",
       " '00 wednesday',\n",
       " '00 went',\n",
       " '00 west',\n",
       " '00 wheat',\n",
       " '00 wicker',\n",
       " '00 winner',\n",
       " '00 wonder',\n",
       " '00 work',\n",
       " '00 wors',\n",
       " '00 wow',\n",
       " '00 xmen',\n",
       " '00 yea',\n",
       " '00 year',\n",
       " '00 zero',\n",
       " '000',\n",
       " '000 00',\n",
       " '000 000',\n",
       " '000 0000',\n",
       " '000 0000001',\n",
       " '000 00025',\n",
       " '000 000can',\n",
       " '000 000r',\n",
       " '000 000s',\n",
       " '000 001',\n",
       " '000 003',\n",
       " '000 005',\n",
       " '000 007',\n",
       " '000 015',\n",
       " '000 030',\n",
       " '000 10',\n",
       " '000 100',\n",
       " '000 1000',\n",
       " '000 10k',\n",
       " '000 11',\n",
       " '000 110m2',\n",
       " '000 1141',\n",
       " '000 12',\n",
       " '000 120',\n",
       " '000 1200',\n",
       " '000 146',\n",
       " '000 15',\n",
       " '000 150',\n",
       " '000 190',\n",
       " '000 1940s',\n",
       " '000 1960',\n",
       " '000 1989',\n",
       " '000 1990',\n",
       " '000 1994',\n",
       " '000 20',\n",
       " '000 200',\n",
       " '000 2000',\n",
       " '000 2010',\n",
       " '000 20102050',\n",
       " '000 2013',\n",
       " '000 2017',\n",
       " '000 23',\n",
       " '000 2359',\n",
       " '000 2400',\n",
       " '000 25',\n",
       " '000 268',\n",
       " '000 27',\n",
       " '000 27k',\n",
       " '000 30',\n",
       " '000 300',\n",
       " '000 3000',\n",
       " '000 311usa',\n",
       " '000 319',\n",
       " '000 36turkey',\n",
       " '000 39',\n",
       " '000 3p',\n",
       " '000 40',\n",
       " '000 400',\n",
       " '000 4000',\n",
       " '000 45',\n",
       " '000 450',\n",
       " '000 50',\n",
       " '000 500',\n",
       " '000 519000',\n",
       " '000 522',\n",
       " '000 53',\n",
       " '000 585000',\n",
       " '000 600',\n",
       " '000 700',\n",
       " '000 7000',\n",
       " '000 725i',\n",
       " '000 750',\n",
       " '000 80',\n",
       " '000 911',\n",
       " '000 931000',\n",
       " '000 955000',\n",
       " '000 abort',\n",
       " '000 accept',\n",
       " '000 accord',\n",
       " '000 account',\n",
       " '000 acr',\n",
       " '000 activ',\n",
       " '000 actual',\n",
       " '000 ad',\n",
       " '000 addict',\n",
       " '000 addit',\n",
       " '000 african',\n",
       " '000 agreement',\n",
       " '000 ah',\n",
       " '000 aircraft',\n",
       " '000 albanian',\n",
       " '000 albin',\n",
       " '000 alcohol',\n",
       " '000 alien',\n",
       " '000 alli',\n",
       " '000 allow',\n",
       " '000 amaz',\n",
       " '000 amend',\n",
       " '000 american',\n",
       " '000 ancestor',\n",
       " '000 annual',\n",
       " '000 annum',\n",
       " '000 answer',\n",
       " '000 antart',\n",
       " '000 anzac',\n",
       " '000 area',\n",
       " '000 armenian',\n",
       " '000 articl',\n",
       " '000 ask',\n",
       " '000 assum',\n",
       " '000 asylum',\n",
       " '000 aud',\n",
       " '000 australia',\n",
       " '000 bad',\n",
       " '000 bank',\n",
       " '000 basic',\n",
       " '000 battl',\n",
       " '000 bc',\n",
       " '000 bce',\n",
       " '000 becquerel',\n",
       " '000 belgian',\n",
       " '000 best',\n",
       " '000 bicycl',\n",
       " '000 big',\n",
       " '000 biljon',\n",
       " '000 billion',\n",
       " '000 birdshot',\n",
       " '000 birth',\n",
       " '000 birthright',\n",
       " '000 birthsgood',\n",
       " '000 bit',\n",
       " '000 block',\n",
       " '000 bonsai',\n",
       " '000 book',\n",
       " '000 bosnians51',\n",
       " '000 bought',\n",
       " '000 brazil',\n",
       " '000 bremen',\n",
       " '000 bribe',\n",
       " '000 british',\n",
       " '000 briton',\n",
       " '000 btc',\n",
       " '000 buck',\n",
       " '000 bucket',\n",
       " '000 bulgarian',\n",
       " '000 buy',\n",
       " '000 bystand',\n",
       " '000 cabl',\n",
       " '000 cal',\n",
       " '000 calori',\n",
       " '000 calssecond',\n",
       " '000 camp',\n",
       " '000 canadian',\n",
       " '000 cancer',\n",
       " '000 capit',\n",
       " '000 capita',\n",
       " '000 car',\n",
       " '000 care',\n",
       " '000 carri',\n",
       " '000 case',\n",
       " '000 cash',\n",
       " '000 casualti',\n",
       " '000 categori',\n",
       " '000 cavalri',\n",
       " '000 challeng',\n",
       " '000 chanc',\n",
       " '000 channel',\n",
       " '000 chechen',\n",
       " '000 check',\n",
       " '000 childhood',\n",
       " '000 children',\n",
       " '000 chilean',\n",
       " '000 chines',\n",
       " '000 christian',\n",
       " '000 citi',\n",
       " '000 citiz',\n",
       " '000 citizen',\n",
       " '000 citizensi',\n",
       " '000 civilian',\n",
       " '000 closet',\n",
       " '000 coder',\n",
       " '000 come',\n",
       " '000 comment',\n",
       " '000 common',\n",
       " '000 communist',\n",
       " '000 compani',\n",
       " '000 comput',\n",
       " '000 confid',\n",
       " '000 confirm',\n",
       " '000 conquistador',\n",
       " '000 conscript',\n",
       " '000 contain',\n",
       " '000 contrast',\n",
       " '000 convert',\n",
       " '000 corps',\n",
       " '000 count',\n",
       " '000 countgtit',\n",
       " '000 countri',\n",
       " '000 crime',\n",
       " '000 crisi',\n",
       " '000 croatian',\n",
       " '000 crowd',\n",
       " '000 czech',\n",
       " '000 danger',\n",
       " '000 data',\n",
       " '000 day',\n",
       " '000 daysi',\n",
       " '000 de',\n",
       " '000 dead',\n",
       " '000 deal',\n",
       " '000 death',\n",
       " '000 debt',\n",
       " '000 decid',\n",
       " '000 decor',\n",
       " '000 definit',\n",
       " '000 degre',\n",
       " '000 delay',\n",
       " '000 delet',\n",
       " '000 demandsno',\n",
       " '000 demonstrat',\n",
       " '000 depemdan',\n",
       " '000 depend',\n",
       " '000 deporte',\n",
       " '000 devastatedtireddaz',\n",
       " '000 develop',\n",
       " '000 diarrhoea',\n",
       " '000 didbut',\n",
       " '000 die',\n",
       " '000 differ',\n",
       " '000 dinar',\n",
       " '000 direct',\n",
       " '000 displac',\n",
       " '000 dive',\n",
       " '000 doable131',\n",
       " '000 dollar',\n",
       " '000 dtp',\n",
       " '000 dude2',\n",
       " '000 dutch',\n",
       " '000 earli',\n",
       " '000 effect',\n",
       " '000 elig',\n",
       " '000 embrac',\n",
       " '000 emerg',\n",
       " '000 emigr',\n",
       " '000 employe',\n",
       " '000 end',\n",
       " '000 endang',\n",
       " '000 engag',\n",
       " '000 engin',\n",
       " '000 entir',\n",
       " '000 estim',\n",
       " '000 ethnic',\n",
       " '000 eu',\n",
       " '000 eur',\n",
       " '000 eurher',\n",
       " '000 euro',\n",
       " '000 european',\n",
       " '000 everybodi',\n",
       " '000 exampl',\n",
       " '000 excess',\n",
       " '000 exchang',\n",
       " '000 exclus',\n",
       " '000 export',\n",
       " '000 extra',\n",
       " '000 extrem',\n",
       " '000 fan',\n",
       " '000 fanat',\n",
       " '000 far',\n",
       " '000 farm',\n",
       " '000 farmer',\n",
       " '000 fellow',\n",
       " '000 fiftysix',\n",
       " '000 figur',\n",
       " '000 fine',\n",
       " '000 fineor',\n",
       " '000 finland',\n",
       " '000 finlndar',\n",
       " '000 finn',\n",
       " '000 finns141',\n",
       " '000 fire',\n",
       " '000 firearm',\n",
       " '000 fit',\n",
       " '000 flighthour',\n",
       " '000 florida',\n",
       " '000 food',\n",
       " '000 footnot',\n",
       " '000 foreign',\n",
       " '000 foreignalien',\n",
       " '000 forev',\n",
       " '000 fortenit',\n",
       " '000 forther',\n",
       " '000 found',\n",
       " '000 fps',\n",
       " '000 french',\n",
       " '000 friday',\n",
       " '000 ft',\n",
       " '000 fuck',\n",
       " '000 fuckin',\n",
       " '000 fuel',\n",
       " '000 gain',\n",
       " '000 gallon',\n",
       " '000 gdp',\n",
       " '000 generat',\n",
       " '000 german',\n",
       " '000 germani',\n",
       " '000 germans48',\n",
       " '000 germanstoday',\n",
       " '000 given',\n",
       " '000 gone',\n",
       " '000 gop',\n",
       " '000 gpm',\n",
       " '000 grant',\n",
       " '000 gross',\n",
       " '000 group',\n",
       " '000 gulag',\n",
       " '000 gun',\n",
       " '000 guy',\n",
       " '000 gwh',\n",
       " '000 ha',\n",
       " '000 half',\n",
       " '000 hand',\n",
       " '000 harki',\n",
       " '000 hate',\n",
       " '000 heart',\n",
       " '000 heat',\n",
       " '000 hectar',\n",
       " '000 hektar',\n",
       " '000 hell',\n",
       " '000 hello',\n",
       " '000 help',\n",
       " '000 high',\n",
       " '000 hippo',\n",
       " '000 hold',\n",
       " '000 holi',\n",
       " '000 holiday',\n",
       " '000 homeless',\n",
       " '000 hour',\n",
       " '000 huge',\n",
       " '000 hungari',\n",
       " '000 hungarian',\n",
       " '000 ignor',\n",
       " '000 ili',\n",
       " '000 immedi',\n",
       " '000 immigr',\n",
       " '000 imperium',\n",
       " '000 inappropri',\n",
       " '000 incid',\n",
       " '000 includ',\n",
       " '000 increas',\n",
       " '000 incred',\n",
       " '000 indic',\n",
       " '000 individu',\n",
       " '000 inflationthat',\n",
       " '000 inhabit',\n",
       " '000 insid',\n",
       " '000 internet',\n",
       " '000 invest',\n",
       " '000 iranians66',\n",
       " '000 iraqis91',\n",
       " '000 isra',\n",
       " '000 itali',\n",
       " '000 italian',\n",
       " '000 janaug',\n",
       " '000 jeti',\n",
       " '000 jew',\n",
       " '000 jewish',\n",
       " '000 job',\n",
       " '000 jon',\n",
       " '000 judt',\n",
       " '000 karma',\n",
       " '000 kcal',\n",
       " '000 kcalday',\n",
       " '000 kg',\n",
       " '000 kgi',\n",
       " '000 kill',\n",
       " '000 kilo',\n",
       " '000 kilogram',\n",
       " '000 kinda',\n",
       " '000 km',\n",
       " '000 km2',\n",
       " '000 kms',\n",
       " '000 kmthey',\n",
       " '000 know',\n",
       " '000 kosovan',\n",
       " '000 kronor',\n",
       " '000 kuna',\n",
       " '000 kurd',\n",
       " '000 lake',\n",
       " '000 languag',\n",
       " '000 later',\n",
       " '000 latest',\n",
       " '000 latvian',\n",
       " '000 lawyer',\n",
       " '000 left',\n",
       " '000 legal',\n",
       " '000 lei',\n",
       " '000 lel',\n",
       " '000 life',\n",
       " '000 light',\n",
       " '000 like',\n",
       " '000 likelihood',\n",
       " '000 line',\n",
       " '000 link',\n",
       " '000 listen',\n",
       " '000 liter',\n",
       " '000 lithuanian',\n",
       " '000 live',\n",
       " '000 log102',\n",
       " '000 london',\n",
       " '000 long',\n",
       " '000 loot',\n",
       " '000 loss',\n",
       " '000 lost',\n",
       " '000 lot',\n",
       " '000 love',\n",
       " '000 low',\n",
       " '000 lower',\n",
       " '000 lt',\n",
       " '000 lux',\n",
       " '000 luxdirect',\n",
       " '000 luxemburg',\n",
       " '000 ly',\n",
       " '000 m2',\n",
       " '000 m22',\n",
       " '000 macedonian',\n",
       " '000 mah',\n",
       " '000 main',\n",
       " '000 make',\n",
       " '000 malaria',\n",
       " '000 male',\n",
       " '000 man',\n",
       " '000 manag',\n",
       " '000 map',\n",
       " '000 math',\n",
       " '000 mauritius',\n",
       " '000 mayb',\n",
       " '000 mb',\n",
       " '000 mbts',\n",
       " '000 member',\n",
       " '000 men',\n",
       " '000 microsoft',\n",
       " '000 middl',\n",
       " '000 midnight',\n",
       " '000 mile',\n",
       " '000 militia',\n",
       " '000 miljard',\n",
       " '000 miljon',\n",
       " '000 miljoonait',\n",
       " '000 million',\n",
       " '000 miniscul',\n",
       " '000 minor',\n",
       " '000 minut',\n",
       " '000 mistak',\n",
       " '000 mmoll',\n",
       " '000 monkey',\n",
       " '000 month',\n",
       " '000 moos',\n",
       " '000 morehonest',\n",
       " '000 moroccan',\n",
       " '000 morrocan',\n",
       " '000 move',\n",
       " '000 movi',\n",
       " '000 muslim',\n",
       " '000 na',\n",
       " '000 nativ',\n",
       " '000 near',\n",
       " '000 need',\n",
       " '000 net',\n",
       " '000 neuron',\n",
       " '000 new',\n",
       " '000 nis',\n",
       " '000 nok',\n",
       " '000 nomin',\n",
       " '000 noncitizens2011',\n",
       " '000 noneuropean',\n",
       " '000 nonwhit',\n",
       " '000 normal',\n",
       " '000 north',\n",
       " '000 northeastern',\n",
       " '000 note',\n",
       " '000 nothingi',\n",
       " '000 notori',\n",
       " '000 nowaday',\n",
       " '000 nt',\n",
       " '000 number',\n",
       " '000 offici',\n",
       " '000 oh',\n",
       " '000 old',\n",
       " '000 oper',\n",
       " '000 origin',\n",
       " '000 page',\n",
       " '000 palestinian',\n",
       " '000 parchment',\n",
       " '000 particip',\n",
       " '000 pascal',\n",
       " '000 pay',\n",
       " '000 pc',\n",
       " '000 peasant',\n",
       " '000 pend',\n",
       " '000 pengo',\n",
       " '000 peopl',\n",
       " '000 peoplealso',\n",
       " '000 peopleand',\n",
       " '000 peoplebuahahaha45000',\n",
       " '000 peoplehad',\n",
       " '000 peopleontario',\n",
       " '000 peopleso',\n",
       " '000 peoplewhich',\n",
       " '000 perman',\n",
       " '000 person',\n",
       " '000 personnel',\n",
       " '000 personyear',\n",
       " '000 peryeargt',\n",
       " '000 peso',\n",
       " '000 piec',\n",
       " '000 play',\n",
       " '000 pln',\n",
       " '000 plus',\n",
       " '000 plz',\n",
       " '000 podesta',\n",
       " '000 point',\n",
       " '000 pole',\n",
       " '000 poles74',\n",
       " '000 polic',\n",
       " '000 policemen',\n",
       " '000 polish',\n",
       " '000 pontic',\n",
       " '000 pop',\n",
       " '000 popul',\n",
       " '000 populationmi',\n",
       " '000 posit',\n",
       " '000 pound',\n",
       " '000 power',\n",
       " '000 ppl',\n",
       " '000 preorder',\n",
       " '000 present',\n",
       " '000 pretti',\n",
       " '000 priest',\n",
       " '000 prize',\n",
       " '000 profit',\n",
       " '000 proteincod',\n",
       " '000 rais',\n",
       " '000 rakver',\n",
       " '000 random',\n",
       " '000 rang',\n",
       " '000 rarest',\n",
       " '000 rate',\n",
       " '000 reaction',\n",
       " '000 real',\n",
       " '000 realiti',\n",
       " '000 rebel',\n",
       " '000 receiv',\n",
       " '000 recent',\n",
       " '000 recogn',\n",
       " '000 refug',\n",
       " '000 refuge',\n",
       " '000 regist',\n",
       " '000 regular',\n",
       " '000 reject',\n",
       " '000 relat',\n",
       " '000 remark',\n",
       " '000 renault',\n",
       " '000 repetit',\n",
       " '000 report',\n",
       " '000 request',\n",
       " '000 reserv',\n",
       " '000 resettl',\n",
       " '000 resid',\n",
       " '000 respons',\n",
       " '000 retir',\n",
       " '000 return',\n",
       " '000 right',\n",
       " '000 risk',\n",
       " '000 roubl',\n",
       " '000 rough',\n",
       " '000 roughlyfr',\n",
       " '000 round',\n",
       " '000 rubl',\n",
       " '000 run',\n",
       " '000 russian',\n",
       " '000 russianspeak',\n",
       " '000 said',\n",
       " '000 sampl',\n",
       " '000 scientif',\n",
       " '000 scream',\n",
       " '000 screen',\n",
       " '000 second',\n",
       " '000 select',\n",
       " '000 sell',\n",
       " '000 serb',\n",
       " '000 serial',\n",
       " '000 serious',\n",
       " '000 session',\n",
       " '000 sever',\n",
       " '000 shade',\n",
       " '000 share',\n",
       " '000 shit',\n",
       " '000 short',\n",
       " '000 sign',\n",
       " '000 signatur',\n",
       " '000 similar',\n",
       " '000 singl',\n",
       " '000 sita',\n",
       " '000 site',\n",
       " '000 slav',\n",
       " '000 smaller',\n",
       " '000 soldier',\n",
       " '000 somali',\n",
       " '000 somewhat',\n",
       " '000 son',\n",
       " '000 sorri',\n",
       " '000 sound',\n",
       " '000 sourc',\n",
       " '000 south',\n",
       " '000 soviet',\n",
       " '000 soyboy',\n",
       " '000 spain',\n",
       " '000 speaker',\n",
       " '000 speci',\n",
       " '000 spent',\n",
       " '000 spider',\n",
       " '000 sq',\n",
       " '000 squar',\n",
       " '000 srba',\n",
       " '000 start',\n",
       " '000 state',\n",
       " '000 stay',\n",
       " '000 steal',\n",
       " '000 steam',\n",
       " '000 step',\n",
       " '000 storag',\n",
       " '000 stori',\n",
       " '000 storm',\n",
       " '000 strong',\n",
       " '000 student',\n",
       " '000 studi',\n",
       " '000 subject',\n",
       " '000 subscrib',\n",
       " '000 success',\n",
       " '000 surviv',\n",
       " '000 suspect',\n",
       " '000 swede',\n",
       " '000 swim',\n",
       " '000 syria',\n",
       " '000 syrians151',\n",
       " '000 t34s',\n",
       " '000 tabqa',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = train_term_tfidf[0, :].toarray()\n",
    "\n",
    "# compute the most common value\n",
    "c = Counter(v[0])\n",
    "print(c.most_common(1))\n",
    "\n",
    "print(f\"min: {v.min()}\")\n",
    "print(f\"avg: {v.mean()}\")\n",
    "print(f\"max: {v.max()}\")\n",
    "print(f\"std: {v.std()}\")\n",
    "print(f\"mode: {c.most_common(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"tfidf_vectors.pickle\", \"w\") as file:\n",
    "    pickle.dump(tfidf_vectors, file)\n",
    "\n",
    "with open(\"train_term_tfidf.pickle\", \"w\") as file:\n",
    "    pickle.dump(train_term_tfidf, file)\n",
    "\n",
    "with open(\"val_term_tfidf.pickle\", \"w\") as file:\n",
    "    pickle.dump(val_term_tfidf, file)\n",
    "\n",
    "with open(\"test_term_tfidf.pickle\", \"w\") as file:\n",
    "    pickle.dump(test_term_tfidf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [word for word in comments_posts_tokenized_df['body'].values]\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWV = Word2Vec(sentences, workers = 3, min_count=5, window = 10, size = EMBEDDING_DIM)\n",
    "modelWV.train(sentences, total_examples=len(sentences), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
