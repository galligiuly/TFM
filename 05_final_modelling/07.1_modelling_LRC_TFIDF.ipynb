{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_modelling_LRC-TFIDF.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2ATPnUFBaSVd"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozSLLVc8WxuM",
        "colab_type": "text"
      },
      "source": [
        "# Train First Model: Logistic Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOvhuG5eWub4",
        "colab_type": "code",
        "outputId": "ad221af1-0f7b-4c35-f166-ab2873c0b176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaX5cOcVdFxB",
        "colab_type": "code",
        "outputId": "38a6e70d-d9ae-4cbc-aa21-4b645faa1455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "!gcloud auth login"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?code_challenge=JNHO3GLLTYo1IImvEU5Y9l0u5Z7oZo-6u8QJeC-obCo&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
            "\n",
            "\n",
            "Enter verification code: 4/tgG5uez5FLp7JMciT4mT2fE_mLFjVtU6E2_qwQA28XqpCKDmEGbKp7o\n",
            "\u001b[1;33mWARNING:\u001b[0m `gcloud auth login` no longer writes application default credentials.\n",
            "If you need to use ADC, see:\n",
            "  gcloud auth application-default --help\n",
            "\n",
            "You are now logged in as [galli.giuly@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wbRqGSudFvM",
        "colab_type": "code",
        "outputId": "3d97c288-8064-4fa3-8f76-bfdd202025e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!gcloud config set project reddit-master"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdvJn3Z-dFsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cabcb355-9a18-4476-d311-20fcf3255884"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import nltk.data\n",
        "import nltk\n",
        "import ast\n",
        "import logging\n",
        "nltk.download('punkt')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import precision_score, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCb6DWYzilR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subreddits = ['aww', 'nba', 'movies', 'todayilearned', 'IAmA', 'Fitness', 'worldnews', 'technology', 'europe', 'politics', 'atheism','science', 'funny', 'gaming']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aorNGgxiilNj",
        "colab_type": "code",
        "outputId": "3fef9664-77fc-44e6-b956-a4d83a3f6975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSCS0yqlilAp",
        "colab_type": "code",
        "outputId": "f2142353-40c8-42ba-c51b-60d44ccf1979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model_LR_df = pd.DataFrame(columns = [\"subreddit\", \"body\", \"subreddit_id\"])\n",
        "\n",
        "for subreddit in subreddits:\n",
        "    !gsutil cp gs://reddit_final_results/comments_posts_{subreddit}.pkl .\n",
        "    print(\"comments_posts_\" + subreddit + \".pkl downloaded\")\n",
        "\n",
        "    df = pd.read_pickle(\"comments_posts_\" + subreddit + \".pkl\")\n",
        "\n",
        "    model_LR_df = pd.concat([model_LR_df, df], ignore_index=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_aww.pkl...\n",
            "- [1 files][ 63.8 MiB/ 63.8 MiB]                                                \n",
            "Operation completed over 1 objects/63.8 MiB.                                     \n",
            "comments_posts_aww.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_nba.pkl...\n",
            "- [1 files][ 88.3 MiB/ 88.3 MiB]                                                \n",
            "Operation completed over 1 objects/88.3 MiB.                                     \n",
            "comments_posts_nba.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_movies.pkl...\n",
            "- [1 files][103.2 MiB/103.2 MiB]                                                \n",
            "Operation completed over 1 objects/103.2 MiB.                                    \n",
            "comments_posts_movies.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_todayilearned.pkl...\n",
            "- [1 files][101.1 MiB/101.1 MiB]                                                \n",
            "Operation completed over 1 objects/101.1 MiB.                                    \n",
            "comments_posts_todayilearned.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_IAmA.pkl...\n",
            "- [1 files][132.8 MiB/132.8 MiB]                                                \n",
            "Operation completed over 1 objects/132.8 MiB.                                    \n",
            "comments_posts_IAmA.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_Fitness.pkl...\n",
            "\\ [1 files][144.0 MiB/144.0 MiB]                                                \n",
            "Operation completed over 1 objects/144.0 MiB.                                    \n",
            "comments_posts_Fitness.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_worldnews.pkl...\n",
            "- [1 files][134.8 MiB/134.8 MiB]                                                \n",
            "Operation completed over 1 objects/134.8 MiB.                                    \n",
            "comments_posts_worldnews.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_technology.pkl...\n",
            "/ [1 files][136.2 MiB/136.2 MiB]                                                \n",
            "Operation completed over 1 objects/136.2 MiB.                                    \n",
            "comments_posts_technology.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_europe.pkl...\n",
            "| [1 files][128.4 MiB/128.4 MiB]                                                \n",
            "Operation completed over 1 objects/128.4 MiB.                                    \n",
            "comments_posts_europe.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_politics.pkl...\n",
            "| [1 files][118.7 MiB/118.7 MiB]                                                \n",
            "Operation completed over 1 objects/118.7 MiB.                                    \n",
            "comments_posts_politics.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_atheism.pkl...\n",
            "| [1 files][144.0 MiB/144.0 MiB]                                                \n",
            "Operation completed over 1 objects/144.0 MiB.                                    \n",
            "comments_posts_atheism.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_science.pkl...\n",
            "| [1 files][147.5 MiB/147.5 MiB]                                                \n",
            "Operation completed over 1 objects/147.5 MiB.                                    \n",
            "comments_posts_science.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_funny.pkl...\n",
            "\\ [1 files][ 69.3 MiB/ 69.3 MiB]                                                \n",
            "Operation completed over 1 objects/69.3 MiB.                                     \n",
            "comments_posts_funny.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_gaming.pkl...\n",
            "- [1 files][ 79.4 MiB/ 79.4 MiB]                                                \n",
            "Operation completed over 1 objects/79.4 MiB.                                     \n",
            "comments_posts_gaming.pkl downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 53 s, sys: 13.2 s, total: 1min 6s\n",
            "Wall time: 1min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnrNZSm2ik-e",
        "colab_type": "code",
        "outputId": "5ea82eb6-1e81-48a2-e2ee-02f31520f3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_LR_df.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10833350, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGRDlAK_ik8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = model_LR_df['body']\n",
        "y = model_LR_df['subreddit_id']\n",
        "\n",
        "# Definint a fucntion that slit the dataset into three subsets: train, val and test\n",
        "def train_dev_test_split(X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42, stratify=y_val)\n",
        "    return dict(X_train=X_train, \n",
        "                X_val=X_val, \n",
        "                X_test=X_test, \n",
        "                y_train=y_train, \n",
        "                y_val=y_val, \n",
        "                y_test=y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dlSE8kpik6B",
        "colab_type": "code",
        "outputId": "180022b1-36cd-4344-cbd8-a8af70375bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%time\n",
        "\n",
        "model_LR_df_split = train_dev_test_split(X,y)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.2 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLNoV6fzvADf",
        "colab_type": "code",
        "outputId": "62ffd82b-0613-4fd8-a21c-238669a9efba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_LR_df_split.keys()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['X_train', 'X_val', 'X_test', 'y_train', 'y_val', 'y_test'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEyWAOpIik3l",
        "colab_type": "code",
        "outputId": "4b8ae768-5591-4fe6-95d7-b9d4b2e1234d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "parameters = [\n",
        "    {\n",
        "        'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "        'tfidf__ngram_range': [(1,1),(1,2),(1,3)],\n",
        "        'clf__estimator__class_weight': [\"balanced\", None]\n",
        "    }\n",
        "]\n",
        "\n",
        "pipeline = Pipeline([\n",
        "  ('tfidf', TfidfVectorizer(preprocessor=' '.join)),\n",
        "  ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
        "])\n",
        "\n",
        "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=1, verbose=10)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 334 µs, sys: 38 µs, total: 372 µs\n",
            "Wall time: 380 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joGY0GSdRUSV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b18eef60-37f5-4c54-8862-23240b9348d0"
      },
      "source": [
        "y.iloc[5055564]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5UgWsaSik17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1f0ef8ff-b249-445e-ed8b-b8ad70fc93b2"
      },
      "source": [
        "%%time\n",
        "\n",
        "grid_search_tune.fit(model_LR_df_split['X_train'], model_LR_df_split['y_train'].to_numpy().astype('int'))\n",
        "            \n",
        "logging.info('Best parameters set:')\n",
        "print(grid_search_tune.best_estimator_.steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] clf__estimator__class_weight=balanced, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfM5VDCLikz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# measuring performance on test set\n",
        "logging.info('Applying best classifier on test data:')\n",
        "best_clf = grid_search_tune.best_estimator_\n",
        "predictions = best_clf.predict(model_LR_df_split['X_test'])\n",
        "print(classification_report(model_LR_df_split['y_test'], predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGC6K7nTikxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAGpDyWsikvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jqEI5y0iktg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p10bObIeikpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmpZDloNikkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfS2ogbdiki_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFoXhnBCikgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WerkqgRcikeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "offmA4hnikbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85g9dLrRikZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJMAItLSikXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRjpLRe3ikUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WijO0l8YikNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD8B6NL2dsaC",
        "colab_type": "code",
        "outputId": "d1ab6fb7-f834-48fb-c347-54867e0c2243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# importing the data - atheism\n",
        "!gsutil cp gs://reddit_final_results/comments_posts_atheism.pkl ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_final_results/comments_posts_atheism.pkl...\n",
            "\\ [1 files][144.0 MiB/144.0 MiB]                                                \n",
            "Operation completed over 1 objects/144.0 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3M1FWm-eTcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_atheism = pd.read_pickle(\"/content/comments_posts_atheism.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOGkhxzIdyNs",
        "colab_type": "code",
        "outputId": "9500d644-2b4e-4ad6-f2eb-0559f0582df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# importing the tfidf vectorizer for atheism \n",
        "!gsutil cp gs://reddit_models/content/tfidf_atheism"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CommandException: Wrong number of arguments for \"cp\" command.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWKXqfQodFmm",
        "colab_type": "code",
        "outputId": "06e290ba-7f24-48a7-e47b-73137505e8b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# importing the fasttext vectorizer for atheism \n",
        "!gsutil cp gs://reddit_models/content/model_fasttext_atheism"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CommandException: Wrong number of arguments for \"cp\" command.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYfO0KzDdFXO",
        "colab_type": "code",
        "outputId": "45452b50-fc16-4f92-f883-4ec364dd1f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "subreddits = list(df_atheism['subreddit'].values)\n",
        "subreddits"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Unnamed: 0.1', 'subreddit', 'body', 'subreddit_id']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ATPnUFBaSVd",
        "colab_type": "text"
      },
      "source": [
        "## Train, test and validation - atheism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjE3jK2edFkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_atheism = df_atheism['body']\n",
        "y_atheism = df_atheism['subreddit_id']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi7TrEihanUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definint a fucntion that slit the dataset into three subsets: train, val and test\n",
        "def train_dev_test_split(X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42, stratify=y_val)\n",
        "    return dict(X_train=X_train, \n",
        "                X_val=X_val, \n",
        "                X_test=X_test, \n",
        "                y_train=y_train, \n",
        "                y_val=y_val, \n",
        "                y_test=y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCJo9_KranMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_atheism_split = train_dev_test_split(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU4cRsswam-F",
        "colab_type": "code",
        "outputId": "47c74033-a444-411a-fdef-329194e67597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_atheism_split.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['X_train', 'X_val', 'X_test', 'y_train', 'y_val', 'y_test'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuJvptPKdFhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a pipeline combining a text feature extractor with a Naive Bayes classifier\n",
        "NB_pipeline = Pipeline([\n",
        "                (\"tfidf\", TfidfVectorizer(tokenizer=tokenizer.tokenize)),\n",
        "                (\"clf\", OneVsRestClassifier(MultinomialNB(\n",
        "                    fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print(f\"Processing {category}...\")\n",
        "    NB_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = NB_pipeline.predict(X_valid)\n",
        "    print(f\"Test accuracy: {accuracy_score(validation[category], prediction)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1cAkn7adFfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf = Pipeline([('vectorizer', CountVectorizer()),\n",
        "    ('transformer', TfidfTransformer()),\n",
        "    ('classifier', MultinomialNB())])\n",
        "text_clf.fit(data[\"train\"][\"texts\"],data[\"train\"][\"labels\"])\n",
        "predicted_labels = text_clf.predict(data[\"test\"][\"texts\"])\n",
        "print(np.mean(predicted_labels == data[\"test\"][\"labels\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdPzjsjLdFck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}