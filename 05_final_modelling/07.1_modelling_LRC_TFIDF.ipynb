{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ozSLLVc8WxuM"
   },
   "source": [
    "# Train First Model: Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "iaX5cOcVdFxB",
    "outputId": "38a6e70d-d9ae-4cbc-aa21-4b645faa1455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?code_challenge=LSw7hQSUITaGjuhyYZM8b6X2GeogaypgYRraQW4JTqQ&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
      "\n",
      "\n",
      "\u001b[1;33mWARNING:\u001b[0m `gcloud auth login` no longer writes application default credentials.\n",
      "If you need to use ADC, see:\n",
      "  gcloud auth application-default --help\n",
      "\n",
      "You are now logged in as [galli.giuly@gmail.com].\n",
      "Your current project is [reddit-master].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_wbRqGSudFvM",
    "outputId": "3d97c288-8064-4fa3-8f76-bfdd202025e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project reddit-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NdvJn3Z-dFsL",
    "outputId": "cabcb355-9a18-4476-d311-20fcf3255884"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giuliagalli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import nltk.data\n",
    "import joblib\n",
    "import nltk\n",
    "import ast\n",
    "import logging\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import datetime\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loggg(msg):\n",
    "    print(\"[INFO] {}: {}\".format(datetime.datetime.now(), msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the df is too big to work the GridSearch with all data, I'd like to try it just for 2 subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCb6DWYzilR7"
   },
   "outputs": [],
   "source": [
    "subreddits = ['aww', 'nba']#, 'movies', 'todayilearned', 'IAmA', 'Fitness', 'worldnews', 'technology', 'europe', 'politics', 'atheism','science', 'funny', 'gaming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aorNGgxiilNj",
    "outputId": "3fef9664-77fc-44e6-b956-a4d83a3f6975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07.1_modelling_LRC_TFIDF.ipynb\r\n",
      "07.2_modelling_SVM_TFIDF.ipynb\r\n",
      "07.3_modelling_RandomForestClassifier_TFIDF.ipynb\r\n",
      "Untitled.ipynb\r\n",
      "comments_posts_Fitness.pkl\r\n",
      "comments_posts_IAmA.pkl\r\n",
      "comments_posts_atheism.pkl\r\n",
      "comments_posts_aww.pkl\r\n",
      "comments_posts_europe.pkl\r\n",
      "comments_posts_funny.pkl\r\n",
      "comments_posts_gaming.pkl\r\n",
      "comments_posts_movies.pkl\r\n",
      "comments_posts_nba.pkl\r\n",
      "comments_posts_politics.pkl\r\n",
      "comments_posts_science.pkl\r\n",
      "comments_posts_technology.pkl\r\n",
      "comments_posts_todayilearned.pkl\r\n",
      "comments_posts_worldnews.pkl\r\n",
      "model_svm.joblib\r\n",
      "summary.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vSCS0yqlilAp",
    "outputId": "f2142353-40c8-42ba-c51b-60d44ccf1979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 09:37:09.397511: comments_posts_aww.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 09:37:12.483193: comments_posts_nba.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.61 s, sys: 874 ms, total: 6.49 s\n",
      "Wall time: 6.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_LR_df = pd.DataFrame(columns = [\"subreddit\", \"body\", \"subreddit_id\"])\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    #!gsutil cp gs://reddit_final_results/comments_posts_{subreddit}.pkl .\n",
    "    loggg(\"comments_posts_\" + subreddit + \".pkl downloaded\")\n",
    "\n",
    "    df = pd.read_pickle(\"comments_posts_\" + subreddit + \".pkl\")\n",
    "\n",
    "    model_LR_df = pd.concat([model_LR_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mnrNZSm2ik-e",
    "outputId": "5ea82eb6-1e81-48a2-e2ee-02f31520f3dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:00:31.499115: (1547237, 4)\n"
     ]
    }
   ],
   "source": [
    "loggg(model_LR_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGRDlAK_ik8D"
   },
   "outputs": [],
   "source": [
    "X = model_LR_df['body']\n",
    "y = model_LR_df['subreddit_id']\n",
    "\n",
    "# Definint a fucntion that slit the dataset into three subsets: train, val and test\n",
    "def train_dev_test_split(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42, stratify=y_val)\n",
    "    return dict(X_train=X_train,\n",
    "                X_val=X_val,\n",
    "                X_test=X_test,\n",
    "                y_train=y_train.astype('int'),\n",
    "                y_val=y_val.astype('int'),\n",
    "                y_test=y_test.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "\"tfidf__max_df\": [0.25, 0.5, 0.75],\n",
    "\"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "\"clf__estimator__class_weight\": [\"balanced\",None]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR_df_split = train_dev_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=\" \".join, ngram_range=(1,2))),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag', class_weight=\"balanced\", multi_class='auto')))\n",
    "    ])\n",
    "\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:01:28.141049: pipeline created\n",
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed: 13.5min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed: 15.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2',\n",
       "        preprocessor=<built-in meth...ate=None,\n",
       "          solver='sag', tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'tfidf__max_df': [0.25, 0.5, 0.75], 'tfidf__ngram_range': [(1, 1), (1, 2)], 'clf__estimator__class_weight': ['balanced', None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loggg(\"pipeline created\")\n",
    "\n",
    "grid_search_tune.fit(model_LR_df_split['X_train'], model_LR_df_split['y_train'].to_numpy().astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:23:55.262423: Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2',\n",
      "        preprocessor=<built-in method join of str object at 0x109c58e30>,\n",
      "        smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "        sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, use_idf=True, vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='auto', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='sag', tol=0.0001, verbose=0, warm_start=False),\n",
      "          n_jobs=None))]\n",
      "[INFO] 2019-11-27 10:23:55.263601: Applying best classifier on test data:\n"
     ]
    }
   ],
   "source": [
    "# measuring performance on test set\n",
    "loggg('Best parameters set:')\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "# measuring performance on test set\n",
    "loggg('Applying best classifier on test data:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.87      0.90      0.89    116073\n",
      "           8       0.90      0.87      0.88    116013\n",
      "\n",
      "   micro avg       0.89      0.89      0.89    232086\n",
      "   macro avg       0.89      0.89      0.89    232086\n",
      "weighted avg       0.89      0.89      0.89    232086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_clf = grid_search_tune.best_estimator_\n",
    "predictions = best_clf.predict(model_LR_df_split['X_test'])\n",
    "print(classification_report(model_LR_df_split['y_test'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data\n",
    "\n",
    "Now we try to train all the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['aww', 'nba', 'movies', 'todayilearned', 'IAmA', 'Fitness', 'worldnews', 'technology', 'europe', 'politics', 'atheism','science', 'funny', 'gaming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:27:37.118482: comments_posts_aww.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:27:39.817402: comments_posts_nba.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:27:43.227748: comments_posts_movies.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:27:47.487481: comments_posts_todayilearned.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:27:53.789155: comments_posts_IAmA.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:28:06.033133: comments_posts_Fitness.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:28:13.549698: comments_posts_worldnews.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:28:53.825680: comments_posts_technology.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:29:01.570925: comments_posts_europe.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:29:56.139000: comments_posts_politics.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:30:04.026640: comments_posts_atheism.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:30:17.630814: comments_posts_science.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:31:41.434750: comments_posts_funny.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:31:50.569051: comments_posts_gaming.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 2min 5s, total: 3min 54s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_LR_df = pd.DataFrame(columns = [\"subreddit\", \"body\", \"subreddit_id\"])\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    #!gsutil cp gs://reddit_final_results/comments_posts_{subreddit}.pkl .\n",
    "    loggg(\"comments_posts_\" + subreddit + \".pkl downloaded\")\n",
    "\n",
    "    df = pd.read_pickle(\"comments_posts_\" + subreddit + \".pkl\")\n",
    "\n",
    "    model_LR_df = pd.concat([model_LR_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:32:18.892564: (10833350, 4)\n"
     ]
    }
   ],
   "source": [
    "loggg(model_LR_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_LR_df['body']\n",
    "y = model_LR_df['subreddit_id']\n",
    "\n",
    "# Definint a fucntion that slit the dataset into three subsets: train and test\n",
    "def train_dev_test_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    return dict(X_train=X_train,\n",
    "                X_test=X_test,\n",
    "                y_train=y_train.astype('int'),\n",
    "                y_test=y_test.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1dlSE8kpik6B",
    "outputId": "180022b1-36cd-4344-cbd8-a8af70375bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 14.3 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "model_LR_df_split = train_dev_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jLNoV6fzvADf",
    "outputId": "62ffd82b-0613-4fd8-a21c-238669a9efba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_train', 'X_test', 'y_train', 'y_test'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR_df_split.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, 13, 10,  3, 12,  5,  7,  8,  4,  2,  9,  6, 11])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR_df_split['y_train'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_LR = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=\" \".join, ngram_range=(1,2))),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag', class_weight=\"balanced\", multi_class='auto')))\n",
    "    ])\n",
    "\n",
    "\n",
    "model_LR.fit(model_LR_df_split['X_train'], model_LR_df_split['y_train'])\n",
    "\n",
    "joblib.dump(model_LR, \"model_lr.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model_lr.joblib [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1 files][341.8 MiB/341.8 MiB]    7.8 MiB/s                                   \n",
      "Operation completed over 1 objects/341.8 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!/Users/giuliagalli/google-cloud-sdk/bin/gsutil cp model_lr.joblib gs://reddit_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfM5VDCLikz2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:51:41.972883: prediction done\n"
     ]
    }
   ],
   "source": [
    "prediction = model_lr.predict(model_LR_df_split['X_test'])\n",
    "\n",
    "loggg(\"prediction done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGC6K7nTikxO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-27 10:52:12.195361: reporting Logistic Regression results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.81      0.77    232289\n",
      "           1       0.38      0.29      0.33    232243\n",
      "           2       0.54      0.55      0.55    232080\n",
      "           3       0.50      0.59      0.55    232145\n",
      "           4       0.53      0.53      0.53    232007\n",
      "           5       0.24      0.31      0.27    231810\n",
      "           6       0.53      0.53      0.53    232122\n",
      "           7       0.58      0.58      0.58    232173\n",
      "           8       0.70      0.71      0.70    232026\n",
      "           9       0.48      0.51      0.50    232104\n",
      "          10       0.44      0.52      0.47    232721\n",
      "          11       0.47      0.48      0.47    232131\n",
      "          12       0.28      0.20      0.23    231997\n",
      "          13       0.38      0.27      0.31    232157\n",
      "\n",
      "   micro avg       0.49      0.49      0.49   3250005\n",
      "   macro avg       0.49      0.49      0.49   3250005\n",
      "weighted avg       0.49      0.49      0.49   3250005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loggg(\"reporting Logistic Regression results\")\n",
    "\n",
    "print(classification_report(model_LR_df_split['y_test'], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAGpDyWsikvZ"
   },
   "outputs": [],
   "source": [
    "# gaming => correct\n",
    "\n",
    "sentence1 = \"What games would you like to discuss this week? [Game Thread Voting]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1jqEI5y0iktg"
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "text1 = tokenizer.tokenize(sentence1)\n",
    "\n",
    "result1 = model_lr.predict([text1])\n",
    "\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmpZDloNikkm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10059610</th>\n",
       "      <td>477973.0</td>\n",
       "      <td>[sad, gotten, point, fan, better, game, aaa, s...</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059611</th>\n",
       "      <td>477974.0</td>\n",
       "      <td>[pencil, refer]</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059612</th>\n",
       "      <td>477975.0</td>\n",
       "      <td>[nake, gun, 33, 13, mayb]</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059613</th>\n",
       "      <td>477976.0</td>\n",
       "      <td>[knight, old, republ, game]</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059614</th>\n",
       "      <td>477977.0</td>\n",
       "      <td>[seen, redead]</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0.1                                               body  \\\n",
       "10059610      477973.0  [sad, gotten, point, fan, better, game, aaa, s...   \n",
       "10059611      477974.0                                    [pencil, refer]   \n",
       "10059612      477975.0                          [nake, gun, 33, 13, mayb]   \n",
       "10059613      477976.0                        [knight, old, republ, game]   \n",
       "10059614      477977.0                                     [seen, redead]   \n",
       "\n",
       "         subreddit subreddit_id  \n",
       "10059610    gaming            6  \n",
       "10059611    gaming            6  \n",
       "10059612    gaming            6  \n",
       "10059613    gaming            6  \n",
       "10059614    gaming            6  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR_df[model_LR_df['subreddit_id'] == int(result1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfS2ogbdiki_"
   },
   "outputs": [],
   "source": [
    "# aww => wrong\n",
    "\n",
    "sentence2 = \"God bless you for introducing me to this sub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFoXhnBCikgg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "text2 = tokenizer.tokenize(sentence2)\n",
    "\n",
    "result2 = model_lr.predict([text2])\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7737574</th>\n",
       "      <td>88650.0</td>\n",
       "      <td>[argu, deep, human, except, thing, defianc, idea]</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737575</th>\n",
       "      <td>88651.0</td>\n",
       "      <td>[differ, harri, potter, bibl, coupl, billion, ...</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737576</th>\n",
       "      <td>88652.0</td>\n",
       "      <td>[explain, argument]</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737577</th>\n",
       "      <td>88653.0</td>\n",
       "      <td>[driven, selfish]</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737578</th>\n",
       "      <td>88654.0</td>\n",
       "      <td>[hear, god, heal,  , sarcasm]</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                                               body  \\\n",
       "7737574       88650.0  [argu, deep, human, except, thing, defianc, idea]   \n",
       "7737575       88651.0  [differ, harri, potter, bibl, coupl, billion, ...   \n",
       "7737576       88652.0                                [explain, argument]   \n",
       "7737577       88653.0                                  [driven, selfish]   \n",
       "7737578       88654.0                      [hear, god, heal,  , sarcasm]   \n",
       "\n",
       "        subreddit subreddit_id  \n",
       "7737574   atheism            2  \n",
       "7737575   atheism            2  \n",
       "7737576   atheism            2  \n",
       "7737577   atheism            2  \n",
       "7737578   atheism            2  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR_df[model_LR_df['subreddit_id'] == int(result2)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todayilearned => correct\n",
    "\n",
    "sentence3 = \"TIL that conservationists created a match.com profile for the world’s loneliest frog named Romeo, in order to raise funds for expeditions to find more of his species. Their work paid off, and Romeo and Juliet are together at last.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "text3 = tokenizer.tokenize(sentence3)\n",
    "\n",
    "result3 = model_lr.predict([text3])\n",
    "\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2321146</th>\n",
       "      <td>13706.0</td>\n",
       "      <td>[ve, known, kill, box, corn, pop, meself]</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321147</th>\n",
       "      <td>13707.0</td>\n",
       "      <td>[gtit, like, random, battleground, dayboth, be...</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321148</th>\n",
       "      <td>13708.0</td>\n",
       "      <td>[ultim, beet, beetus]</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321149</th>\n",
       "      <td>13709.0</td>\n",
       "      <td>[salt, fat, scum, rise, salt, pork, salt, beef...</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321150</th>\n",
       "      <td>13710.0</td>\n",
       "      <td>[guy, right, need, correct, answer, know, nich...</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                                               body  \\\n",
       "2321146       13706.0          [ve, known, kill, box, corn, pop, meself]   \n",
       "2321147       13707.0  [gtit, like, random, battleground, dayboth, be...   \n",
       "2321148       13708.0                              [ultim, beet, beetus]   \n",
       "2321149       13709.0  [salt, fat, scum, rise, salt, pork, salt, beef...   \n",
       "2321150       13710.0  [guy, right, need, correct, answer, know, nich...   \n",
       "\n",
       "             subreddit subreddit_id  \n",
       "2321146  todayilearned           12  \n",
       "2321147  todayilearned           12  \n",
       "2321148  todayilearned           12  \n",
       "2321149  todayilearned           12  \n",
       "2321150  todayilearned           12  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR_df[model_LR_df['subreddit_id'] == int(result3)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAMA => wrong\n",
    "\n",
    "sentence4 =\"I do medical billing for a major hospital and I believe for-profit health care is evil. AMAA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "text4 = tokenizer.tokenize(sentence4)\n",
    "\n",
    "result4 = model_lr.predict([text4])\n",
    "\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2321146</th>\n",
       "      <td>13706.0</td>\n",
       "      <td>[ve, known, kill, box, corn, pop, meself]</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321147</th>\n",
       "      <td>13707.0</td>\n",
       "      <td>[gtit, like, random, battleground, dayboth, be...</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321148</th>\n",
       "      <td>13708.0</td>\n",
       "      <td>[ultim, beet, beetus]</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321149</th>\n",
       "      <td>13709.0</td>\n",
       "      <td>[salt, fat, scum, rise, salt, pork, salt, beef...</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321150</th>\n",
       "      <td>13710.0</td>\n",
       "      <td>[guy, right, need, correct, answer, know, nich...</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                                               body  \\\n",
       "2321146       13706.0          [ve, known, kill, box, corn, pop, meself]   \n",
       "2321147       13707.0  [gtit, like, random, battleground, dayboth, be...   \n",
       "2321148       13708.0                              [ultim, beet, beetus]   \n",
       "2321149       13709.0  [salt, fat, scum, rise, salt, pork, salt, beef...   \n",
       "2321150       13710.0  [guy, right, need, correct, answer, know, nich...   \n",
       "\n",
       "             subreddit subreddit_id  \n",
       "2321146  todayilearned           12  \n",
       "2321147  todayilearned           12  \n",
       "2321148  todayilearned           12  \n",
       "2321149  todayilearned           12  \n",
       "2321150  todayilearned           12  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR_df[model_LR_df['subreddit_id'] == int(result4)].head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2ATPnUFBaSVd"
   ],
   "machine_shape": "hm",
   "name": "07_modelling_LRC-TFIDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
