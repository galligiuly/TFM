{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?code_challenge=eftJqPgA2maXRTwooAygvQrQwkgPTiaA-L4ziACPewY&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
      "\n",
      "\n",
      "\u001b[1;33mWARNING:\u001b[0m `gcloud auth login` no longer writes application default credentials.\n",
      "If you need to use ADC, see:\n",
      "  gcloud auth application-default --help\n",
      "\n",
      "You are now logged in as [galli.giuly@gmail.com].\n",
      "Your current project is [reddit-master].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project reddit-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://reddit_final_results/comments_posts_2018.csv\r\n",
      "gs://reddit_final_results/comments_posts_2018.zip\r\n",
      "gs://reddit_final_results/comments_posts_2018_V2.csv\r\n",
      "gs://reddit_final_results/comments_posts_2018_V2.zip\r\n",
      "gs://reddit_final_results/comments_posts_Fitness.pkl\r\n",
      "gs://reddit_final_results/comments_posts_IAmA.pkl\r\n",
      "gs://reddit_final_results/comments_posts_atheism.pkl\r\n",
      "gs://reddit_final_results/comments_posts_aww.pkl\r\n",
      "gs://reddit_final_results/comments_posts_europe.pkl\r\n",
      "gs://reddit_final_results/comments_posts_funny.pkl\r\n",
      "gs://reddit_final_results/comments_posts_gaming.pkl\r\n",
      "gs://reddit_final_results/comments_posts_movies.pkl\r\n",
      "gs://reddit_final_results/comments_posts_nba.pkl\r\n",
      "gs://reddit_final_results/comments_posts_politics.pkl\r\n",
      "gs://reddit_final_results/comments_posts_science.pkl\r\n",
      "gs://reddit_final_results/comments_posts_technology.pkl\r\n",
      "gs://reddit_final_results/comments_posts_todayilearned.pkl\r\n",
      "gs://reddit_final_results/comments_posts_tokenized.csv\r\n",
      "gs://reddit_final_results/comments_posts_tokenized.zip\r\n",
      "gs://reddit_final_results/comments_posts_tokenized_V2.csv\r\n",
      "gs://reddit_final_results/comments_posts_tokenized_V2.zip\r\n",
      "gs://reddit_final_results/comments_posts_tokenized_df.pkl\r\n",
      "gs://reddit_final_results/comments_posts_tokenized_df_gzip.pkl\r\n",
      "gs://reddit_final_results/comments_posts_worldnews.pkl\r\n",
      "gs://reddit_final_results/red_comments_posts.csv\r\n",
      "gs://reddit_final_results/red_comments_posts_tokenized.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://reddit_final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pepe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import nltk.data\n",
    "import joblib\n",
    "import nltk\n",
    "import ast\n",
    "import logging\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "import datetime\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loggg(msg):\n",
    "    print(\"[INFO] {}: {}\".format(datetime.datetime.now(), msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['aww', 'nba', 'movies', 'todayilearned', 'IAmA', 'Fitness', 'worldnews', 'technology', 'europe', 'politics', 'atheism','science', 'funny', 'gaming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-12-01 16:13:14.018013: comments_posts_aww.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:15.436744: comments_posts_nba.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:17.100632: comments_posts_movies.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:19.296291: comments_posts_todayilearned.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:21.342605: comments_posts_IAmA.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:24.011856: comments_posts_Fitness.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:27.211075: comments_posts_worldnews.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:30.583880: comments_posts_technology.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:32.807254: comments_posts_europe.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:36.618931: comments_posts_politics.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:38.753457: comments_posts_atheism.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:43.345518: comments_posts_science.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:46.013548: comments_posts_funny.pkl downloaded\n",
      "[INFO] 2019-12-01 16:13:47.815133: comments_posts_gaming.pkl downloaded\n"
     ]
    }
   ],
   "source": [
    "model_RFC_df = pd.DataFrame(columns = [\"subreddit\", \"body\", \"subreddit_id\"])\n",
    "\n",
    "for subreddit in subreddits: \n",
    "    #!gsutil cp gs://reddit_final_results/comments_posts_{subreddit}.pkl .\n",
    "    loggg(\"comments_posts_\" + subreddit + \".pkl downloaded\")\n",
    "    df = pd.read_pickle(\"comments_posts_\" + subreddit + \".pkl\")\n",
    "    model_RFC_df = pd.concat([model_RFC_df, df], ignore_index=True, sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-12-01 16:14:02.816284: (10833350, 4)\n"
     ]
    }
   ],
   "source": [
    "X = model_RFC_df['body']\n",
    "y = model_RFC_df['subreddit_id']\n",
    "\n",
    "def train_dev_test_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    return dict(X_train=X_train, \n",
    "                X_test=X_test, \n",
    "                y_train=y_train.astype('int'), \n",
    "                y_test=y_test.astype('int'))\n",
    "\n",
    "loggg(model_RFC_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RFC_df_split = train_dev_test_split(X,y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=\" \".join, ngram_range=(1,1), max_df=0.5)),\n",
    "    (\"clf\", RandomForestClassifier(min_samples_leaf=1,max_depth=30,n_estimators=500,n_jobs=4))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.5, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=<built-in method join of str object at 0x7feca9deb970>,\n",
       "                                 smooth_idf=True, stop_words=None,\n",
       "                                 strip_acce...\n",
       "                ('clf',\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=30,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=500, n_jobs=4,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(model_RFC_df_split['X_train'], model_RFC_df_split['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65    232289\n",
      "           1       0.44      0.09      0.15    232243\n",
      "           2       0.51      0.50      0.50    232080\n",
      "           3       0.20      0.76      0.32    232145\n",
      "           4       0.50      0.42      0.46    232007\n",
      "           5       0.13      0.19      0.16    231810\n",
      "           6       0.53      0.37      0.44    232122\n",
      "           7       0.59      0.46      0.51    232173\n",
      "           8       0.63      0.61      0.62    232026\n",
      "           9       0.47      0.41      0.44    232104\n",
      "          10       0.39      0.39      0.39    232721\n",
      "          11       0.45      0.36      0.40    232131\n",
      "          12       0.32      0.03      0.06    231997\n",
      "          13       0.43      0.12      0.19    232157\n",
      "\n",
      "    accuracy                           0.39   3250005\n",
      "   macro avg       0.44      0.39      0.38   3250005\n",
      "weighted avg       0.44      0.39      0.38   3250005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(model_RFC_df_split['X_test'])\n",
    "print(classification_report(model_RFC_df_split['y_test'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_rfc.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline, 'model_rfc.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model_rfc.joblib [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "| [1 files][588.4 MiB/588.4 MiB]    4.8 MiB/s                                   \n",
      "Operation completed over 1 objects/588.4 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp model_rfc.joblib gs://reddit_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
