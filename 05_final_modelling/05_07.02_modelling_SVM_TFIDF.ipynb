{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Second Model: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?code_challenge=v2pAz1K0SpBbeMVoilQ-BTvGco_67TpSJ8ib0tnLEAA&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
      "\n",
      "\n",
      "\u001b[1;33mWARNING:\u001b[0m `gcloud auth login` no longer writes application default credentials.\n",
      "If you need to use ADC, see:\n",
      "  gcloud auth application-default --help\n",
      "\n",
      "You are now logged in as [galli.giuly@gmail.com].\n",
      "Your current project is [reddit-master].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project reddit-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giuliagalli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import nltk.data\n",
    "import joblib\n",
    "import nltk\n",
    "import ast\n",
    "import logging\n",
    "nltk.download('punkt')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['aww', 'nba', 'movies', 'todayilearned', 'IAmA', 'Fitness', 'worldnews', 'technology', 'europe', 'politics', 'atheism','science', 'funny', 'gaming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_aww.pkl...\n",
      "\\ [1 files][ 63.8 MiB/ 63.8 MiB]    5.9 MiB/s                                   \n",
      "Operation completed over 1 objects/63.8 MiB.                                     \n",
      "comments_posts_aww.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_nba.pkl...\n",
      "/ [1 files][ 88.3 MiB/ 88.3 MiB]    5.9 MiB/s                                   \n",
      "Operation completed over 1 objects/88.3 MiB.                                     \n",
      "comments_posts_nba.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_movies.pkl...\n",
      "- [1 files][103.2 MiB/103.2 MiB]    6.2 MiB/s                                   \n",
      "Operation completed over 1 objects/103.2 MiB.                                    \n",
      "comments_posts_movies.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_todayilearned.pkl...\n",
      "| [1 files][101.1 MiB/101.1 MiB]    6.1 MiB/s                                   \n",
      "Operation completed over 1 objects/101.1 MiB.                                    \n",
      "comments_posts_todayilearned.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_IAmA.pkl...\n",
      "/ [1 files][132.8 MiB/132.8 MiB]    6.9 MiB/s                                   \n",
      "Operation completed over 1 objects/132.8 MiB.                                    \n",
      "comments_posts_IAmA.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_Fitness.pkl...\n",
      "| [1 files][144.0 MiB/144.0 MiB]    6.9 MiB/s                                   \n",
      "Operation completed over 1 objects/144.0 MiB.                                    \n",
      "comments_posts_Fitness.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_worldnews.pkl...\n",
      "| [1 files][134.8 MiB/134.8 MiB]    6.5 MiB/s                                   \n",
      "Operation completed over 1 objects/134.8 MiB.                                    \n",
      "comments_posts_worldnews.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_technology.pkl...\n",
      "\\ [1 files][136.2 MiB/136.2 MiB]    4.7 MiB/s                                   \n",
      "Operation completed over 1 objects/136.2 MiB.                                    \n",
      "comments_posts_technology.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_europe.pkl...\n",
      "- [1 files][128.4 MiB/128.4 MiB]    7.4 MiB/s                                   \n",
      "Operation completed over 1 objects/128.4 MiB.                                    \n",
      "comments_posts_europe.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_politics.pkl...\n",
      "/ [1 files][118.7 MiB/118.7 MiB]    7.4 MiB/s                                   \n",
      "Operation completed over 1 objects/118.7 MiB.                                    \n",
      "comments_posts_politics.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_atheism.pkl...\n",
      "/ [1 files][144.0 MiB/144.0 MiB]    7.1 MiB/s                                   \n",
      "Operation completed over 1 objects/144.0 MiB.                                    \n",
      "comments_posts_atheism.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_science.pkl...\n",
      "| [1 files][147.5 MiB/147.5 MiB]    4.2 MiB/s                                   \n",
      "Operation completed over 1 objects/147.5 MiB.                                    \n",
      "comments_posts_science.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_funny.pkl...\n",
      "| [1 files][ 69.3 MiB/ 69.3 MiB]    5.8 MiB/s                                   \n",
      "Operation completed over 1 objects/69.3 MiB.                                     \n",
      "comments_posts_funny.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/comments_posts_gaming.pkl...\n",
      "- [1 files][ 79.4 MiB/ 79.4 MiB]    6.5 MiB/s                                   \n",
      "Operation completed over 1 objects/79.4 MiB.                                     \n",
      "comments_posts_gaming.pkl downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 4min 9s, total: 6min 49s\n",
      "Wall time: 14min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_SVM_df = pd.DataFrame(columns = [\"subreddit\", \"body\", \"subreddit_id\"])\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    !/Users/giuliagalli/google-cloud-sdk/bin/gsutil cp gs://reddit_final_results/comments_posts_{subreddit}.pkl .\n",
    "    print(\"comments_posts_\" + subreddit + \".pkl downloaded\")\n",
    "    \n",
    "    df = pd.read_pickle(\"comments_posts_\" + subreddit + \".pkl\")\n",
    "\n",
    "    model_SVM_df = pd.concat([model_SVM_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10833350, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SVM_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_SVM_df['body']\n",
    "y = model_SVM_df['subreddit_id']\n",
    "\n",
    "# Definint a fucntion that slit the dataset into three subsets: train, val and test\n",
    "def train_dev_test_split(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42, stratify=y_val)\n",
    "    return dict(X_train=X_train, \n",
    "                X_val=X_val, \n",
    "                X_test=X_test, \n",
    "                y_train=y_train.astype('int'), \n",
    "                y_val=y_val.astype('int'), \n",
    "                y_test=y_test.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.9 s, sys: 9.53 s, total: 42.4 s\n",
      "Wall time: 44.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_SVM_df_split = train_dev_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_train', 'X_val', 'X_test', 'y_train', 'y_val', 'y_test'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SVM_df_split.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, 13, 10,  3, 12,  5,  7,  8,  4,  2,  9,  6, 11])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SVM_df_split['y_train'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 48min 35s, sys: 49min 45s, total: 2h 38min 21s\n",
      "Wall time: 4h 17min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_svm = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_df=0.5, ngram_range=(1,2), preprocessor=' '.join)),\n",
    "            ('clf', LinearSVC(), n_jobs=1),\n",
    "            ])\n",
    "\n",
    "model_svm.fit(model_SVM_df_split['X_train'], model_SVM_df_split['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_svm.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model_svm, \"model_svm.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model_svm.joblib [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\ [1 files][  4.6 GiB/  4.6 GiB]    6.5 MiB/s                                   \n",
      "Operation completed over 1 objects/4.6 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!/Users/giuliagalli/google-cloud-sdk/bin/gsutil cp model_svm.joblib gs://reddit_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 41s, sys: 7min 1s, total: 11min 43s\n",
      "Wall time: 18min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prediction = model_svm.predict(model_SVM_df_split['X_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.78    116144\n",
      "           1       0.40      0.34      0.37    116121\n",
      "           2       0.53      0.55      0.54    116040\n",
      "           3       0.49      0.61      0.54    116073\n",
      "           4       0.52      0.53      0.53    116004\n",
      "           5       0.28      0.27      0.27    115905\n",
      "           6       0.55      0.54      0.55    116061\n",
      "           7       0.58      0.61      0.59    116086\n",
      "           8       0.71      0.72      0.72    116013\n",
      "           9       0.48      0.50      0.49    116052\n",
      "          10       0.46      0.50      0.48    116360\n",
      "          11       0.47      0.48      0.47    116066\n",
      "          12       0.30      0.23      0.26    115999\n",
      "          13       0.35      0.30      0.33    116078\n",
      "\n",
      "   micro avg       0.50      0.50      0.50   1625002\n",
      "   macro avg       0.49      0.50      0.49   1625002\n",
      "weighted avg       0.49      0.50      0.49   1625002\n",
      "\n",
      "CPU times: user 3.43 s, sys: 296 ms, total: 3.73 s\n",
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(classification_report(model_SVM_df_split['y_val'], prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funny => wrong\n",
    "\n",
    "sentence1 = \"I designed the Family Feud Fender to quickly remove yourself from family holiday drama.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "text1 = tokenizer.tokenize(sentence1)\n",
    "\n",
    "result1 = model_svm.predict([text1])\n",
    "\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6190536</th>\n",
       "      <td>49079.0</td>\n",
       "      <td>[kind, brazilian, move]</td>\n",
       "      <td>europe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190537</th>\n",
       "      <td>49080.0</td>\n",
       "      <td>[trulli, consensu, facto, legal, europa, itali...</td>\n",
       "      <td>europe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190538</th>\n",
       "      <td>49081.0</td>\n",
       "      <td>[dutch, articl, day, ago, link, quot, raad, va...</td>\n",
       "      <td>europe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190539</th>\n",
       "      <td>49082.0</td>\n",
       "      <td>[peru, recent, trust, currenc,  , major, trans...</td>\n",
       "      <td>europe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190540</th>\n",
       "      <td>49083.0</td>\n",
       "      <td>[argument, actual, talk, 1930, voter, afraid, ...</td>\n",
       "      <td>europe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                                               body  \\\n",
       "6190536       49079.0                            [kind, brazilian, move]   \n",
       "6190537       49080.0  [trulli, consensu, facto, legal, europa, itali...   \n",
       "6190538       49081.0  [dutch, articl, day, ago, link, quot, raad, va...   \n",
       "6190539       49082.0  [peru, recent, trust, currenc,  , major, trans...   \n",
       "6190540       49083.0  [argument, actual, talk, 1930, voter, afraid, ...   \n",
       "\n",
       "        subreddit subreddit_id  \n",
       "6190536    europe            4  \n",
       "6190537    europe            4  \n",
       "6190538    europe            4  \n",
       "6190539    europe            4  \n",
       "6190540    europe            4  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SVM_df[model_SVM_df['subreddit_id'] == int(result1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nba => wrong\n",
    "\n",
    "sentence2 = \"It's like feast or famine. It seemed like there were 6 games yesterday and only 2 today and I think tomorrow there is around 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "text2 = tokenizer.tokenize(sentence2)\n",
    "\n",
    "result2 = model_svm.predict([text2])\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7737574</th>\n",
       "      <td>88650.0</td>\n",
       "      <td>[argu, deep, human, except, thing, defianc, idea]</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737575</th>\n",
       "      <td>88651.0</td>\n",
       "      <td>[differ, harri, potter, bibl, coupl, billion, ...</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737576</th>\n",
       "      <td>88652.0</td>\n",
       "      <td>[explain, argument]</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737577</th>\n",
       "      <td>88653.0</td>\n",
       "      <td>[driven, selfish]</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737578</th>\n",
       "      <td>88654.0</td>\n",
       "      <td>[hear, god, heal,  , sarcasm]</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                                               body  \\\n",
       "7737574       88650.0  [argu, deep, human, except, thing, defianc, idea]   \n",
       "7737575       88651.0  [differ, harri, potter, bibl, coupl, billion, ...   \n",
       "7737576       88652.0                                [explain, argument]   \n",
       "7737577       88653.0                                  [driven, selfish]   \n",
       "7737578       88654.0                      [hear, god, heal,  , sarcasm]   \n",
       "\n",
       "        subreddit subreddit_id  \n",
       "7737574   atheism            2  \n",
       "7737575   atheism            2  \n",
       "7737576   atheism            2  \n",
       "7737577   atheism            2  \n",
       "7737578   atheism            2  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SVM_df[model_SVM_df['subreddit_id'] == int(result2)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies => correct\n",
    "\n",
    "sentence3 = \"Netflix takes over lease to iconic but failed 71-year-old movie theater in New York City to show its own original films.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "text3 = tokenizer.tokenize(sentence3)\n",
    "\n",
    "result3 = model_svm.predict([text3])\n",
    "\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1547237</th>\n",
       "      <td>13233.0</td>\n",
       "      <td>[13, minut, snake, eye]</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547238</th>\n",
       "      <td>13234.0</td>\n",
       "      <td>[14,  , grand, budapest, hotel15,  , mad, max,...</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547239</th>\n",
       "      <td>13235.0</td>\n",
       "      <td>[took, 8, peopl, help, ocean, heist]</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547240</th>\n",
       "      <td>13236.0</td>\n",
       "      <td>[remindm, 6, month]</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547241</th>\n",
       "      <td>13237.0</td>\n",
       "      <td>[mean, lot, defend, way, defend, attack, negat...</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                                               body  \\\n",
       "1547237       13233.0                            [13, minut, snake, eye]   \n",
       "1547238       13234.0  [14,  , grand, budapest, hotel15,  , mad, max,...   \n",
       "1547239       13235.0               [took, 8, peopl, help, ocean, heist]   \n",
       "1547240       13236.0                                [remindm, 6, month]   \n",
       "1547241       13237.0  [mean, lot, defend, way, defend, attack, negat...   \n",
       "\n",
       "        subreddit subreddit_id  \n",
       "1547237    movies            7  \n",
       "1547238    movies            7  \n",
       "1547239    movies            7  \n",
       "1547240    movies            7  \n",
       "1547241    movies            7  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SVM_df[model_SVM_df['subreddit_id'] == int(result3)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1547237</th>\n",
       "      <td>13233.0</td>\n",
       "      <td>[13, minut, snake, eye]</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547238</th>\n",
       "      <td>13234.0</td>\n",
       "      <td>[14,  , grand, budapest, hotel15,  , mad, max,...</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547239</th>\n",
       "      <td>13235.0</td>\n",
       "      <td>[took, 8, peopl, help, ocean, heist]</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547240</th>\n",
       "      <td>13236.0</td>\n",
       "      <td>[remindm, 6, month]</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547241</th>\n",
       "      <td>13237.0</td>\n",
       "      <td>[mean, lot, defend, way, defend, attack, negat...</td>\n",
       "      <td>movies</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                                               body  \\\n",
       "1547237       13233.0                            [13, minut, snake, eye]   \n",
       "1547238       13234.0  [14,  , grand, budapest, hotel15,  , mad, max,...   \n",
       "1547239       13235.0               [took, 8, peopl, help, ocean, heist]   \n",
       "1547240       13236.0                                [remindm, 6, month]   \n",
       "1547241       13237.0  [mean, lot, defend, way, defend, attack, negat...   \n",
       "\n",
       "        subreddit subreddit_id  \n",
       "1547237    movies            7  \n",
       "1547238    movies            7  \n",
       "1547239    movies            7  \n",
       "1547240    movies            7  \n",
       "1547241    movies            7  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movies => correct\n",
    "\n",
    "sentence3 = \"Netflix takes over lease to iconic but failed 71-year-old movie theater in New York City to show its own original films.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funny => correct\n",
    "\n",
    "sentence4 = \"Making Friends Monday! Share your game tags here!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "text4 = tokenizer.tokenize(sentence4)\n",
    "\n",
    "result4 = model_svm.predict([text4])\n",
    "\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10059610</th>\n",
       "      <td>477973.0</td>\n",
       "      <td>[sad, gotten, point, fan, better, game, aaa, s...</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059611</th>\n",
       "      <td>477974.0</td>\n",
       "      <td>[pencil, refer]</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059612</th>\n",
       "      <td>477975.0</td>\n",
       "      <td>[nake, gun, 33, 13, mayb]</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059613</th>\n",
       "      <td>477976.0</td>\n",
       "      <td>[knight, old, republ, game]</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059614</th>\n",
       "      <td>477977.0</td>\n",
       "      <td>[seen, redead]</td>\n",
       "      <td>gaming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0.1                                               body  \\\n",
       "10059610      477973.0  [sad, gotten, point, fan, better, game, aaa, s...   \n",
       "10059611      477974.0                                    [pencil, refer]   \n",
       "10059612      477975.0                          [nake, gun, 33, 13, mayb]   \n",
       "10059613      477976.0                        [knight, old, republ, game]   \n",
       "10059614      477977.0                                     [seen, redead]   \n",
       "\n",
       "         subreddit subreddit_id  \n",
       "10059610    gaming            6  \n",
       "10059611    gaming            6  \n",
       "10059612    gaming            6  \n",
       "10059613    gaming            6  \n",
       "10059614    gaming            6  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SVM_df[model_SVM_df['subreddit_id'] == int(result4)].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
