{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?code_challenge=5kHGtuc3vAXO-l-9s3alDQW6ghKp6lZ65LXApWJ8_-Y&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
      "\n",
      "\n",
      "\u001b[1;33mWARNING:\u001b[0m `gcloud auth login` no longer writes application default credentials.\n",
      "If you need to use ADC, see:\n",
      "  gcloud auth application-default --help\n",
      "\n",
      "You are now logged in as [galli.giuly@gmail.com].\n",
      "Your current project is [reddit-master].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project reddit-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pepe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import nltk.data\n",
    "import joblib\n",
    "import nltk\n",
    "import ast\n",
    "import logging\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import datetime\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loggg(msg):\n",
    "    print(\"[INFO] {}: {}\".format(datetime.datetime.now(), msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_model_SVM_df = pd.read_pickle(\"red_comments_posts_tokenized.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>[hell, job, blue, duck, photo, sourc]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>funny</td>\n",
       "      <td>[one, settl]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funny</td>\n",
       "      <td>[grand, father, fish, cod, like, 50, year, ago]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funny</td>\n",
       "      <td>[fake, yes,  , effect, real]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>funny</td>\n",
       "      <td>[smoke, bomb, bug, poison, combin]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                             body  subreddit_id\n",
       "0     funny            [hell, job, blue, duck, photo, sourc]             5\n",
       "1     funny                                     [one, settl]             5\n",
       "2     funny  [grand, father, fish, cod, like, 50, year, ago]             5\n",
       "3     funny                     [fake, yes,  , effect, real]             5\n",
       "4     funny               [smoke, bomb, bug, poison, combin]             5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_model_SVM_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = red_model_SVM_df['body']\n",
    "y = red_model_SVM_df['subreddit_id']\n",
    "\n",
    "# Definint a fucntion that slit the dataset into three subsets: train, val and test\n",
    "def train_dev_test_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    return dict(X_train=X_train,\n",
    "                X_test=X_test,\n",
    "                y_train=y_train.astype('int'),\n",
    "                y_test=y_test.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_model_SVM_df_split = train_dev_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        \"tfidf__max_df\": [0.25, 0.5, 0.75],\n",
    "        \"tfidf__ngram_range\": [(1, 2)],\n",
    "        \"clf__C\": [0.01, 0.1, 1],\n",
    "        \"clf__class_weight\": [\"balanced\"]   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 227 µs, sys: 91 µs, total: 318 µs\n",
      "Wall time: 2.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(preprocessor=' '.join)),\n",
    "            ('clf', LinearSVC()),\n",
    "            ])\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-30 11:29:30.716978: pipeline created\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=2)]: Done  18 out of  18 | elapsed: 31.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=<built-in method join of str object...\n",
       "                                                  loss='squared_hinge',\n",
       "                                                  max_iter=1000,\n",
       "                                                  multi_class='ovr',\n",
       "                                                  penalty='l2',\n",
       "                                                  random_state=None, tol=0.0001,\n",
       "                                                  verbose=0))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=2,\n",
       "             param_grid={'clf__C': [0.01, 0.1, 1],\n",
       "                         'clf__class_weight': ['balanced'],\n",
       "                         'tfidf__max_df': [0.25, 0.5, 0.75],\n",
       "                         'tfidf__ngram_range': [(1, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loggg(\"pipeline created\")\n",
    "\n",
    "grid_search_tune.fit(red_model_SVM_df_split['X_train'], red_model_SVM_df_split['y_train'].to_numpy().astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-30 12:05:19.296374: Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.25, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 2), norm='l2',\n",
      "                preprocessor=<built-in method join of str object at 0x7efc147d4970>,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)), ('clf', LinearSVC(C=0.1, class_weight='balanced', dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0))]\n",
      "[INFO] 2019-11-30 12:05:19.297739: Applying best classifier on test data:\n"
     ]
    }
   ],
   "source": [
    "# measuring performance on test set\n",
    "loggg('Best parameters set:')\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "# measuring performance on test set\n",
    "loggg('Applying best classifier on test data:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80     46567\n",
      "           1       0.45      0.36      0.40     46567\n",
      "           2       0.57      0.62      0.60     46567\n",
      "           3       0.55      0.67      0.60     46097\n",
      "           4       0.56      0.59      0.57     46567\n",
      "           5       0.33      0.26      0.29     46567\n",
      "           6       0.60      0.61      0.61     46567\n",
      "           7       0.62      0.68      0.65     46568\n",
      "           8       0.76      0.82      0.78     46567\n",
      "           9       0.48      0.50      0.49     46257\n",
      "          10       0.46      0.57      0.50     46125\n",
      "          11       0.51      0.57      0.54     46164\n",
      "          12       0.35      0.19      0.25     46093\n",
      "          13       0.40      0.28      0.33     46098\n",
      "\n",
      "    accuracy                           0.54    649371\n",
      "   macro avg       0.53      0.54      0.53    649371\n",
      "weighted avg       0.53      0.54      0.53    649371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_clf = grid_search_tune.best_estimator_\n",
    "predictions = best_clf.predict(red_model_SVM_df_split['X_test'])\n",
    "print(classification_report(red_model_SVM_df_split['y_test'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('red_model_svm.pkl', 'wb') as file:\n",
    "    pkl.dump(best_clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://red_model_svm.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\ [1 files][  1.7 GiB/  1.7 GiB]    3.8 MiB/s                                   \n",
      "Operation completed over 1 objects/1.7 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp red_model_svm.pkl gs://reddit_models/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
