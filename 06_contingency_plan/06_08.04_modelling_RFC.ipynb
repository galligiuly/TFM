{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?code_challenge=HpNtqybVF7xsemovMd8cH67dg5tLajopX44Npp5QP1g&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
      "\n",
      "\n",
      "\u001b[1;33mWARNING:\u001b[0m `gcloud auth login` no longer writes application default credentials.\n",
      "If you need to use ADC, see:\n",
      "  gcloud auth application-default --help\n",
      "\n",
      "You are now logged in as [galli.giuly@gmail.com].\n",
      "Your current project is [reddit-master].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project reddit-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import nltk.data\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, classification_report, confusion_matrix, accuracy_score\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loggg(msg):\n",
    "    print(\"[INFO] {}: {}\".format(datetime.datetime.now(), msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/red_comments_posts_tokenized.pkl...\n",
      "\\ [1 files][358.1 MiB/358.1 MiB]    2.6 MiB/s                                   \n",
      "Operation completed over 1 objects/358.1 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://reddit_final_results/red_comments_posts_tokenized.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_model_RF_df = pd.read_pickle(\"red_comments_posts_tokenized.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>[hell, job, blue, duck, photo, sourc]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>funny</td>\n",
       "      <td>[one, settl]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funny</td>\n",
       "      <td>[grand, father, fish, cod, like, 50, year, ago]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funny</td>\n",
       "      <td>[fake, yes,  , effect, real]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>funny</td>\n",
       "      <td>[smoke, bomb, bug, poison, combin]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                             body  subreddit_id\n",
       "0     funny            [hell, job, blue, duck, photo, sourc]             5\n",
       "1     funny                                     [one, settl]             5\n",
       "2     funny  [grand, father, fish, cod, like, 50, year, ago]             5\n",
       "3     funny                     [fake, yes,  , effect, real]             5\n",
       "4     funny               [smoke, bomb, bug, poison, combin]             5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_model_RF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = red_model_RF_df['body']\n",
    "y = red_model_RF_df['subreddit_id']\n",
    "\n",
    "# Definint a fucntion that slit the dataset into three subsets: train, val and test\n",
    "def train_dev_test_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    return dict(X_train=X_train,\n",
    "                X_test=X_test,\n",
    "                y_train=y_train.astype('int'),\n",
    "                y_test=y_test.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_model_RF_df_split = train_dev_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    #'clf__max_depth': [40, 70, 100],\n",
    "    #'clf__max_features': ['auto'],\n",
    "    'clf__n_estimators': [50, 100, 200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   6 | elapsed:  6.1min remaining: 12.2min\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   6 | elapsed:  9.1min remaining:  9.1min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   6 | elapsed: 10.3min remaining:  5.1min\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed: 20.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed: 20.1min finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'red_model_rfc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'red_model_rfc' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(preprocessor=' '.join)),\n",
    "    (\"clf\", RandomForestClassifier(min_samples_leaf=30))\n",
    "])\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=4, verbose=10)\n",
    "\n",
    "grid_search_tune.fit(red_model_RF_df_split['X_train'], red_model_RF_df_split['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__n_estimators': 200}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=<built-in method join of str object at 0x7f0fa463e970>,\n",
       "                                 smooth_idf=True, stop_words=None,\n",
       "                                 strip_acce...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=30,\n",
       "                                        min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=200, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('red_model_rfc.pkl', 'wb') as file:\n",
    "    pkl.dump(grid_search_tune.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44588496024941954"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.84      0.68     46567\n",
      "           1       0.37      0.16      0.22     46567\n",
      "           2       0.50      0.57      0.53     46567\n",
      "           3       0.47      0.60      0.53     46097\n",
      "           4       0.47      0.50      0.49     46567\n",
      "           5       0.18      0.29      0.22     46567\n",
      "           6       0.57      0.49      0.53     46567\n",
      "           7       0.58      0.58      0.58     46568\n",
      "           8       0.62      0.76      0.68     46567\n",
      "           9       0.42      0.45      0.43     46257\n",
      "          10       0.38      0.44      0.40     46125\n",
      "          11       0.44      0.47      0.45     46164\n",
      "          12       0.24      0.04      0.07     46093\n",
      "          13       0.36      0.14      0.20     46098\n",
      "\n",
      "    accuracy                           0.45    649371\n",
      "   macro avg       0.44      0.45      0.43    649371\n",
      "weighted avg       0.44      0.45      0.43    649371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('red_model_rfc.pkl', 'rb') as file:\n",
    "    model = pkl.load(file)\n",
    "\n",
    "predictions = model.predict(red_model_RF_df_split['X_test'])\n",
    "print(classification_report(red_model_RF_df_split['y_test'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://red_model_rfc.pkl [Content-Type=application/octet-stream]...\n",
      "| [1 files][101.0 MiB/101.0 MiB]    4.8 MiB/s                                   \n",
      "Operation completed over 1 objects/101.0 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# Uploading the model\n",
    "\n",
    "!gsutil cp red_model_rfc.pkl gs://reddit_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_models/red_model_rfc.pkl...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Downloading the model to test it\n",
    "\n",
    "!gsutil cp gs://reddit_models/red_model_rfc.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pd.read_pickle(\"red_model_rfc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=<built-in method join of str object at 0x7f0fa463e970>,\n",
       "                                 smooth_idf=True, stop_words=None,\n",
       "                                 strip_acce...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=30,\n",
       "                                        min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=200, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed downloads\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import re\n",
    "\n",
    "parser = English()\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "\n",
    "# Cleaning the text\n",
    "## remove urls\n",
    "def remove_urls(text):\n",
    "    clean = re.compile(r'http\\S+')\n",
    "    return re.sub(clean, '', str(text))\n",
    "\n",
    "## remove line endings\n",
    "def remove_line_endings (text):\n",
    "    clean = re.compile(r'\\n')\n",
    "    return re.sub(clean, '', str(text))\n",
    "\n",
    "## remove symbols\n",
    "def remove_symbols (text):\n",
    "    clean = re.compile(r\"[^a-zA-Z0-9' ]\") \n",
    "    return re.sub(clean, '', str(text))\n",
    "\n",
    "def clean_text(text):\n",
    "    return remove_urls(remove_line_endings(remove_symbols(text.lower())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization and stemming\n",
    "## tokenization\n",
    "def tokenize(text):\n",
    "    return parser(text)\n",
    "\n",
    "## stopwords\n",
    "def remove_stopwords(tokenized):\n",
    "    without_stopwords = []\n",
    "\n",
    "    for token in tokenized:\n",
    "        if token.text not in STOP_WORDS:\n",
    "            without_stopwords.append(token.text)\n",
    "    return without_stopwords\n",
    "\n",
    "## stemming\n",
    "def stem(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    return stemmed_words\n",
    "\n",
    "def pre_process (text):\n",
    "    return stem(remove_stopwords(tokenize(text)))\n",
    "\n",
    "\n",
    "## Text processing\n",
    "def text_process (text):\n",
    "    return pre_process(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence4 = \"TIL: This year, the official Yu Gi Oh tournament hosted by Konami instituted a hygiene clause to rulebook. This allows judges to penalize players with dirty clothing or terrible odor by giving them a loss. Super Smash Bros tournament have also started implementing similar rules.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['til',\n",
       " 'year',\n",
       " 'offici',\n",
       " 'yu',\n",
       " 'gi',\n",
       " 'oh',\n",
       " 'tournament',\n",
       " 'host',\n",
       " 'konami',\n",
       " 'institut',\n",
       " 'hygien',\n",
       " 'claus',\n",
       " 'rulebook',\n",
       " 'allow',\n",
       " 'judg',\n",
       " 'penal',\n",
       " 'player',\n",
       " 'dirti',\n",
       " 'cloth',\n",
       " 'terribl',\n",
       " 'odor',\n",
       " 'give',\n",
       " 'loss',\n",
       " 'super',\n",
       " 'smash',\n",
       " 'bros',\n",
       " 'tournament',\n",
       " 'start',\n",
       " 'implement',\n",
       " 'similar',\n",
       " 'rule']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4 = text_process(sentence4)\n",
    "\n",
    "text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\n"
     ]
    }
   ],
   "source": [
    "result4 = model.predict([text4])\n",
    "\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(red_model_RF_df_split[\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.452533297606453"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_predicted, red_model_RF_df_split[\"y_test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
