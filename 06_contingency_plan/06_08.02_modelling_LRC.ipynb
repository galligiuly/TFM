{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?code_challenge=bpOx0IwIBlOV1ZD--HsIzBbK3pX4dJeUkTs2Wn_dyzc&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
      "\n",
      "\n",
      "\u001b[1;33mWARNING:\u001b[0m `gcloud auth login` no longer writes application default credentials.\n",
      "If you need to use ADC, see:\n",
      "  gcloud auth application-default --help\n",
      "\n",
      "You are now logged in as [galli.giuly@gmail.com].\n",
      "Your current project is [reddit-master].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project reddit-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://reddit_models/model_fasttext_atheism\r\n",
      "gs://reddit_models/model_lr.joblib\r\n",
      "gs://reddit_models/model_svm.joblib\r\n",
      "gs://reddit_models/red_model_lrc.pkl\r\n",
      "gs://reddit_models/red_model_lstm_20_batchsize_150.h5\r\n",
      "gs://reddit_models/red_model_lstm_30_batchsize_512.h5\r\n",
      "gs://reddit_models/fastext_files/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://reddit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pepe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import nltk.data\n",
    "import joblib\n",
    "import nltk\n",
    "import ast\n",
    "import logging\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import datetime\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loggg(msg):\n",
    "    print(\"[INFO] {}: {}\".format(datetime.datetime.now(), msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://reddit_final_results/red_comments_posts_tokenized.pkl...\n",
      "- [1 files][358.1 MiB/358.1 MiB]    3.3 MiB/s                                   \n",
      "Operation completed over 1 objects/358.1 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://reddit_final_results/red_comments_posts_tokenized.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_model_LR_df = pd.read_pickle(\"red_comments_posts_tokenized.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>[hell, job, blue, duck, photo, sourc]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>funny</td>\n",
       "      <td>[one, settl]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funny</td>\n",
       "      <td>[grand, father, fish, cod, like, 50, year, ago]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funny</td>\n",
       "      <td>[fake, yes,  , effect, real]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>funny</td>\n",
       "      <td>[smoke, bomb, bug, poison, combin]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                             body  subreddit_id\n",
       "0     funny            [hell, job, blue, duck, photo, sourc]             5\n",
       "1     funny                                     [one, settl]             5\n",
       "2     funny  [grand, father, fish, cod, like, 50, year, ago]             5\n",
       "3     funny                     [fake, yes,  , effect, real]             5\n",
       "4     funny               [smoke, bomb, bug, poison, combin]             5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_model_LR_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = red_model_LR_df['body']\n",
    "y = red_model_LR_df['subreddit_id']\n",
    "\n",
    "# Definint a fucntion that slit the dataset into three subsets: train, val and test\n",
    "def train_dev_test_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    return dict(X_train=X_train,\n",
    "                X_test=X_test,\n",
    "                y_train=y_train.astype('int'),\n",
    "                y_test=y_test.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_model_LR_df_split = train_dev_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "\"tfidf__max_df\": [0.25, 0.5, 0.75],\n",
    "\"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "\"clf__estimator__class_weight\": [\"balanced\",None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=\" \".join)),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag', multi_class='auto')))\n",
    "    ])\n",
    "\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=4, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-29 23:15:33.056553: pipeline created\n",
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed: 52.0min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed: 92.6min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed: 129.7min\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  24 | elapsed: 133.5min remaining: 26.7min\n",
      "[Parallel(n_jobs=4)]: Done  24 out of  24 | elapsed: 144.2min finished\n",
      "/nix/store/q3c697cny5xlaax78zvhapdslfpq08i5-python3.7-scikit-learn-0.21.3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=<built-in method join of str object...\n",
       "                                                                                         n_jobs=None,\n",
       "                                                                                         penalty='l2',\n",
       "                                                                                         random_state=None,\n",
       "                                                                                         solver='sag',\n",
       "                                                                                         tol=0.0001,\n",
       "                                                                                         verbose=0,\n",
       "                                                                                         warm_start=False),\n",
       "                                                            n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'clf__estimator__class_weight': ['balanced', None],\n",
       "                         'tfidf__max_df': [0.25, 0.5, 0.75],\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loggg(\"pipeline created\")\n",
    "\n",
    "grid_search_tune.fit(red_model_LR_df_split['X_train'], red_model_LR_df_split['y_train'].to_numpy().astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-11-30 09:23:37.165910: Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.75, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 2), norm='l2',\n",
      "                preprocessor=<built-in method join of str object at 0x7f3d7beea970>,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight='balanced',\n",
      "                                                 dual=False, fit_intercept=True,\n",
      "                                                 intercept_scaling=1,\n",
      "                                                 l1_ratio=None, max_iter=100,\n",
      "                                                 multi_class='auto',\n",
      "                                                 n_jobs=None, penalty='l2',\n",
      "                                                 random_state=None,\n",
      "                                                 solver='sag', tol=0.0001,\n",
      "                                                 verbose=0, warm_start=False),\n",
      "                    n_jobs=None))]\n",
      "[INFO] 2019-11-30 09:23:37.167566: Applying best classifier on test data:\n"
     ]
    }
   ],
   "source": [
    "# measuring performance on test set\n",
    "loggg('Best parameters set:')\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "# measuring performance on test set\n",
    "loggg('Applying best classifier on test data:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.75, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=<built-in method join of str object at 0x7f3d7beea970>,\n",
       "                                 smooth_idf=True, stop_words=None,\n",
       "                                 strip_acc...\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(C=1.0,\n",
       "                                                                  class_weight='balanced',\n",
       "                                                                  dual=False,\n",
       "                                                                  fit_intercept=True,\n",
       "                                                                  intercept_scaling=1,\n",
       "                                                                  l1_ratio=None,\n",
       "                                                                  max_iter=100,\n",
       "                                                                  multi_class='auto',\n",
       "                                                                  n_jobs=None,\n",
       "                                                                  penalty='l2',\n",
       "                                                                  random_state=None,\n",
       "                                                                  solver='sag',\n",
       "                                                                  tol=0.0001,\n",
       "                                                                  verbose=0,\n",
       "                                                                  warm_start=False),\n",
       "                                     n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf = grid_search_tune.best_estimator_\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__class_weight': 'balanced',\n",
       " 'tfidf__max_df': 0.75,\n",
       " 'tfidf__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5254567389872479"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83     46567\n",
      "           1       0.43      0.38      0.40     46567\n",
      "           2       0.58      0.61      0.59     46567\n",
      "           3       0.59      0.65      0.62     46097\n",
      "           4       0.57      0.57      0.57     46567\n",
      "           5       0.30      0.31      0.31     46567\n",
      "           6       0.62      0.60      0.61     46567\n",
      "           7       0.64      0.66      0.65     46568\n",
      "           8       0.81      0.80      0.81     46567\n",
      "           9       0.47      0.49      0.48     46257\n",
      "          10       0.47      0.55      0.51     46125\n",
      "          11       0.52      0.56      0.54     46164\n",
      "          12       0.30      0.24      0.26     46093\n",
      "          13       0.37      0.30      0.33     46098\n",
      "\n",
      "    accuracy                           0.54    649371\n",
      "   macro avg       0.53      0.54      0.54    649371\n",
      "weighted avg       0.53      0.54      0.54    649371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = best_clf.predict(red_model_LR_df_split['X_test'])\n",
    "print(classification_report(red_model_LR_df_split['y_test'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('red_model_lrc.pkl', 'wb') as file:\n",
    "    pkl.dump(best_clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://red_model_lrc.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1 files][  1.7 GiB/  1.7 GiB]    4.5 MiB/s                                   \n",
      "Operation completed over 1 objects/1.7 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp red_model_lrc.pkl gs://reddit_models/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
